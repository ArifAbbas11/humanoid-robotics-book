"use strict";(globalThis.webpackChunkhumanoid_robotics_book=globalThis.webpackChunkhumanoid_robotics_book||[]).push([[6894],{8453:(e,n,r)=>{r.d(n,{RP:()=>o});var s=r(6540);const t=s.createContext({});function o(e){const n=s.useContext(t);return s.useMemo(()=>"function"==typeof e?e(n):{...n,...e},[n,e])}},9471:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>l,contentTitle:()=>i,default:()=>m,frontMatter:()=>o,metadata:()=>a,toc:()=>c});var s=r(4848),t=r(8453);const o={},i="LLM Integration",a={id:"vla-integration/llm-integration",title:"LLM Integration",description:"Overview",source:"@site/docs/vla-integration/llm-integration.md",sourceDirName:"vla-integration",slug:"/vla-integration/llm-integration",permalink:"/humanoid-robotics-book/vla-integration/llm-integration",draft:!1,unlisted:!1,editUrl:"https://github.com/ArifAbbas11/humanoid-robotics-book/tree/main/docs/vla-integration/llm-integration.md",tags:[],version:"current",frontMatter:{},sidebar:"bookSidebar",previous:{title:"Voice Recognition",permalink:"/humanoid-robotics-book/vla-integration/voice-recognition"},next:{title:"Cognitive Planning",permalink:"/humanoid-robotics-book/vla-integration/cognitive-planning"}},l={},c=[{value:"Overview",id:"overview",level:2},{value:"LLM Fundamentals",id:"llm-fundamentals",level:2},{value:"What are Large Language Models?",id:"what-are-large-language-models",level:3},{value:"Popular LLM Architectures",id:"popular-llm-architectures",level:3},{value:"LLM Integration Approaches",id:"llm-integration-approaches",level:2},{value:"Cloud-Based APIs",id:"cloud-based-apis",level:3},{value:"Local Deployment",id:"local-deployment",level:3},{value:"ROS 2 Integration Patterns",id:"ros-2-integration-patterns",level:2},{value:"LLM Service Node",id:"llm-service-node",level:3},{value:"Using Open-Source LLMs Locally",id:"using-open-source-llms-locally",level:2},{value:"Ollama Integration",id:"ollama-integration",level:3},{value:"Context-Aware LLM Integration",id:"context-aware-llm-integration",level:2},{value:"Environment Context Integration",id:"environment-context-integration",level:3},{value:"Structured Output for Robot Commands",id:"structured-output-for-robot-commands",level:2},{value:"Command Extraction and Validation",id:"command-extraction-and-validation",level:3},{value:"Performance Optimization",id:"performance-optimization",level:2},{value:"Caching and Batching",id:"caching-and-batching",level:3},{value:"Safety and Security Considerations",id:"safety-and-security-considerations",level:2},{value:"Content Filtering",id:"content-filtering",level:3},{value:"Integration with Vision and Action Systems",id:"integration-with-vision-and-action-systems",level:2},{value:"Multi-Modal Prompt Engineering",id:"multi-modal-prompt-engineering",level:3},{value:"Troubleshooting Common Issues",id:"troubleshooting-common-issues",level:2},{value:"API Connection Problems",id:"api-connection-problems",level:3},{value:"Performance Issues",id:"performance-issues",level:3},{value:"Context Window Limitations",id:"context-window-limitations",level:3},{value:"Best Practices",id:"best-practices",level:2},{value:"System Design",id:"system-design",level:3},{value:"Privacy and Ethics",id:"privacy-and-ethics",level:3},{value:"Next Steps",id:"next-steps",level:2}];function d(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,t.RP)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.h1,{id:"llm-integration",children:"LLM Integration"}),"\n",(0,s.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,s.jsx)(n.p,{children:"Large Language Model (LLM) integration is a crucial component of Vision-Language-Action (VLA) systems, providing the natural language understanding and reasoning capabilities that allow humanoid robots to interpret complex commands and engage in meaningful conversations. This integration bridges human language with robotic action execution."}),"\n",(0,s.jsx)(n.h2,{id:"llm-fundamentals",children:"LLM Fundamentals"}),"\n",(0,s.jsx)(n.h3,{id:"what-are-large-language-models",children:"What are Large Language Models?"}),"\n",(0,s.jsx)(n.p,{children:"Large Language Models are deep learning models trained on vast amounts of text data to understand and generate human-like language. They can perform various natural language tasks including:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Text Generation"}),": Creating coherent, contextually relevant text"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Question Answering"}),": Providing answers to user queries"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Instruction Following"}),": Executing tasks based on natural language instructions"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Reasoning"}),": Performing logical inference and problem solving"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Context Understanding"}),": Maintaining context across conversations"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"popular-llm-architectures",children:"Popular LLM Architectures"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Transformer Architecture"}),": Foundation for most modern LLMs"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"GPT Series"}),": Generative Pre-trained Transformers (OpenAI)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"LLaMA"}),": Open-source models from Meta"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"PaLM"}),": Pathways Language Model from Google"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Claude"}),": Anthropic's conversational AI models"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"llm-integration-approaches",children:"LLM Integration Approaches"}),"\n",(0,s.jsx)(n.h3,{id:"cloud-based-apis",children:"Cloud-Based APIs"}),"\n",(0,s.jsx)(n.p,{children:"Cloud-based LLM services provide easy integration with minimal setup:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"OpenAI API"}),": GPT models with comprehensive documentation"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Anthropic API"}),": Claude models focused on helpful, harmless responses"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Google AI API"}),": PaLM and Gemini models"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"AWS Bedrock"}),": Managed LLM services"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Azure OpenAI"}),": Microsoft's managed OpenAI service"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"local-deployment",children:"Local Deployment"}),"\n",(0,s.jsx)(n.p,{children:"Local deployment provides privacy and reduced latency:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Ollama"}),": Simple local LLM serving"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"vLLM"}),": Fast LLM inference engine"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Hugging Face Transformers"}),": Open-source model library"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"TensorRT-LLM"}),": NVIDIA's optimized inference engine"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"llama.cpp"}),": Lightweight LLM inference in C++"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"ros-2-integration-patterns",children:"ROS 2 Integration Patterns"}),"\n",(0,s.jsx)(n.h3,{id:"llm-service-node",children:"LLM Service Node"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"import rclpy\r\nfrom rclpy.node import Node\r\nfrom std_msgs.msg import String\r\nfrom vla_msgs.srv import LLMQuery  # Custom service\r\nimport openai\r\nimport asyncio\r\nimport threading\r\n\r\nclass LLMIntegrationNode(Node):\r\n    def __init__(self):\r\n        super().__init__('llm_integration_node')\r\n\r\n        # Service for LLM queries\r\n        self.llm_service = self.create_service(\r\n            LLMQuery,\r\n            'llm_query',\r\n            self.handle_llm_request\r\n        )\r\n\r\n        # Publishers for LLM responses\r\n        self.response_pub = self.create_publisher(String, 'llm_response', 10)\r\n\r\n        # Initialize LLM client\r\n        self.llm_client = self.initialize_llm_client()\r\n\r\n        # Conversation history\r\n        self.conversation_history = []\r\n\r\n        self.get_logger().info('LLM Integration Node initialized')\r\n\r\n    def initialize_llm_client(self):\r\n        \"\"\"Initialize the LLM client based on chosen provider\"\"\"\r\n        # Example for OpenAI\r\n        try:\r\n            openai.api_key = self.get_parameter_or_set_default(\r\n                'openai_api_key',\r\n                'your-api-key-here'\r\n            ).value\r\n\r\n            return openai\r\n        except Exception as e:\r\n            self.get_logger().error(f'Failed to initialize LLM client: {e}')\r\n            return None\r\n\r\n    def handle_llm_request(self, request, response):\r\n        \"\"\"Handle LLM query requests\"\"\"\r\n        try:\r\n            # Add to conversation history\r\n            self.conversation_history.append({\r\n                'role': 'user',\r\n                'content': request.query\r\n            })\r\n\r\n            # Generate response from LLM\r\n            llm_response = self.query_llm(\r\n                self.conversation_history,\r\n                request.context\r\n            )\r\n\r\n            # Process the response\r\n            processed_response = self.process_llm_response(llm_response)\r\n\r\n            # Update conversation history\r\n            self.conversation_history.append({\r\n                'role': 'assistant',\r\n                'content': processed_response\r\n            })\r\n\r\n            # Publish response\r\n            response_msg = String()\r\n            response_msg.data = processed_response\r\n            self.response_pub.publish(response_msg)\r\n\r\n            # Set service response\r\n            response.success = True\r\n            response.response = processed_response\r\n\r\n        except Exception as e:\r\n            self.get_logger().error(f'LLM request failed: {e}')\r\n            response.success = False\r\n            response.response = f'Error processing request: {e}'\r\n\r\n        return response\r\n\r\n    def query_llm(self, messages, context=None):\r\n        \"\"\"Query the LLM with conversation history\"\"\"\r\n        try:\r\n            # Prepare messages with context if provided\r\n            full_messages = messages.copy()\r\n\r\n            if context:\r\n                full_messages.insert(0, {\r\n                    'role': 'system',\r\n                    'content': f'Context: {context}\\n\\n'\r\n                              f'You are a helpful assistant for controlling a humanoid robot. '\r\n                              f'Interpret the user\\'s requests and provide appropriate responses. '\r\n                              f'If the user wants to control the robot, respond with structured commands.'\r\n                })\r\n\r\n            # Call the LLM API\r\n            result = self.llm_client.ChatCompletion.create(\r\n                model=\"gpt-3.5-turbo\",  # or gpt-4, Claude, etc.\r\n                messages=full_messages,\r\n                max_tokens=500,\r\n                temperature=0.7\r\n            )\r\n\r\n            return result.choices[0].message.content\r\n\r\n        except Exception as e:\r\n            self.get_logger().error(f'LLM API call failed: {e}')\r\n            return f\"Sorry, I couldn't process your request: {e}\"\r\n\r\n    def process_llm_response(self, response):\r\n        \"\"\"Process and format LLM response for the application\"\"\"\r\n        # Remove any potentially harmful content\r\n        # Format response appropriately\r\n        # Extract structured commands if present\r\n        return response.strip()\n"})}),"\n",(0,s.jsx)(n.h2,{id:"using-open-source-llms-locally",children:"Using Open-Source LLMs Locally"}),"\n",(0,s.jsx)(n.h3,{id:"ollama-integration",children:"Ollama Integration"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"import rclpy\r\nfrom rclpy.node import Node\r\nfrom std_msgs.msg import String\r\nfrom vla_msgs.srv import LLMQuery\r\nimport requests\r\nimport json\r\n\r\nclass OllamaLLMNode(Node):\r\n    def __init__(self):\r\n        super().__init__('ollama_llm_node')\r\n\r\n        # Service for LLM queries\r\n        self.llm_service = self.create_service(\r\n            LLMQuery,\r\n            'ollama_query',\r\n            self.handle_ollama_request\r\n        )\r\n\r\n        # Publisher for responses\r\n        self.response_pub = self.create_publisher(String, 'ollama_response', 10)\r\n\r\n        # Ollama configuration\r\n        self.ollama_url = 'http://localhost:11434/api/generate'\r\n        self.model_name = 'llama2'  # or 'mistral', 'phi', etc.\r\n\r\n        # Test connection\r\n        if self.test_connection():\r\n            self.get_logger().info('Ollama connection established')\r\n        else:\r\n            self.get_logger().error('Failed to connect to Ollama')\r\n\r\n    def test_connection(self):\r\n        \"\"\"Test connection to Ollama server\"\"\"\r\n        try:\r\n            response = requests.get('http://localhost:11434/api/tags')\r\n            return response.status_code == 200\r\n        except Exception:\r\n            return False\r\n\r\n    def handle_ollama_request(self, request, response):\r\n        \"\"\"Handle LLM request using Ollama\"\"\"\r\n        try:\r\n            # Prepare the request payload\r\n            payload = {\r\n                'model': self.model_name,\r\n                'prompt': request.query,\r\n                'stream': False,\r\n                'options': {\r\n                    'temperature': 0.7,\r\n                    'num_ctx': 2048\r\n                }\r\n            }\r\n\r\n            # Send request to Ollama\r\n            result = requests.post(self.ollama_url, json=payload)\r\n\r\n            if result.status_code == 200:\r\n                result_data = result.json()\r\n                llm_response = result_data.get('response', '')\r\n\r\n                # Publish and return response\r\n                response_msg = String()\r\n                response_msg.data = llm_response\r\n                self.response_pub.publish(response_msg)\r\n\r\n                response.success = True\r\n                response.response = llm_response\r\n            else:\r\n                error_msg = f'Ollama request failed: {result.status_code}'\r\n                self.get_logger().error(error_msg)\r\n                response.success = False\r\n                response.response = error_msg\r\n\r\n        except Exception as e:\r\n            error_msg = f'Ollama processing error: {e}'\r\n            self.get_logger().error(error_msg)\r\n            response.success = False\r\n            response.response = error_msg\r\n\r\n        return response\n"})}),"\n",(0,s.jsx)(n.h2,{id:"context-aware-llm-integration",children:"Context-Aware LLM Integration"}),"\n",(0,s.jsx)(n.h3,{id:"environment-context-integration",children:"Environment Context Integration"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"class ContextAwareLLMNode(Node):\r\n    def __init__(self):\r\n        super().__init__('context_aware_llm_node')\r\n\r\n        # Service for contextual queries\r\n        self.contextual_service = self.create_service(\r\n            LLMQuery,\r\n            'contextual_llm_query',\r\n            self.handle_contextual_request\r\n        )\r\n\r\n        # Subscribers for environmental context\r\n        self.vision_sub = self.create_subscription(\r\n            String,  # Simplified - would typically be sensor_msgs\r\n            'vision_context',\r\n            self.vision_callback,\r\n            10\r\n        )\r\n\r\n        self.location_sub = self.create_subscription(\r\n            String,\r\n            'robot_location',\r\n            self.location_callback,\r\n            10\r\n        )\r\n\r\n        # Store environmental context\r\n        self.current_vision_context = \"\"\r\n        self.current_location = \"\"\r\n        self.robot_capabilities = self.get_robot_capabilities()\r\n\r\n    def get_robot_capabilities(self):\r\n        \"\"\"Get robot's current capabilities and limitations\"\"\"\r\n        return {\r\n            'manipulation': True,\r\n            'navigation': True,\r\n            'sensors': ['camera', 'lidar', 'imu'],\r\n            'max_speed': 0.5,\r\n            'weight_limit': 2.0,  # kg\r\n            'reachable_area': 'humanoid_workspace'\r\n        }\r\n\r\n    def vision_callback(self, msg):\r\n        \"\"\"Update vision context\"\"\"\r\n        self.current_vision_context = msg.data\r\n\r\n    def location_callback(self, msg):\r\n        \"\"\"Update location context\"\"\"\r\n        self.current_location = msg.data\r\n\r\n    def handle_contextual_request(self, request, response):\r\n        \"\"\"Handle LLM request with environmental context\"\"\"\r\n        try:\r\n            # Build comprehensive context\r\n            context = self.build_context(request.query)\r\n\r\n            # Query LLM with context\r\n            llm_response = self.query_llm_with_context(\r\n                request.query,\r\n                context\r\n            )\r\n\r\n            # Process and return response\r\n            processed_response = self.process_contextual_response(\r\n                llm_response,\r\n                request.query\r\n            )\r\n\r\n            response.success = True\r\n            response.response = processed_response\r\n\r\n        except Exception as e:\r\n            self.get_logger().error(f'Contextual LLM request failed: {e}')\r\n            response.success = False\r\n            response.response = f'Error: {e}'\r\n\r\n        return response\r\n\r\n    def build_context(self, query):\r\n        \"\"\"Build comprehensive context for the LLM\"\"\"\r\n        context_parts = []\r\n\r\n        # Add location context\r\n        if self.current_location:\r\n            context_parts.append(f\"Robot Location: {self.current_location}\")\r\n\r\n        # Add vision context\r\n        if self.current_vision_context:\r\n            context_parts.append(f\"Current View: {self.current_vision_context}\")\r\n\r\n        # Add robot capabilities\r\n        capabilities = f\"Robot Capabilities: {self.robot_capabilities}\"\r\n        context_parts.append(capabilities)\r\n\r\n        # Add time context\r\n        current_time = self.get_clock().now().to_msg()\r\n        context_parts.append(f\"Current Time: {current_time}\")\r\n\r\n        return \"\\n\".join(context_parts)\r\n\r\n    def query_llm_with_context(self, query, context):\r\n        \"\"\"Query LLM with comprehensive context\"\"\"\r\n        # Prepare messages with context\r\n        messages = [\r\n            {\r\n                'role': 'system',\r\n                'content': f'You are a helpful assistant for a humanoid robot. '\r\n                          f'Context: {context}\\n\\n'\r\n                          f'Robot capabilities: {self.robot_capabilities}\\n\\n'\r\n                          f'Use this information to provide helpful and feasible responses. '\r\n                          f'For action requests, suggest appropriate robot actions when possible.'\r\n            },\r\n            {\r\n                'role': 'user',\r\n                'content': query\r\n            }\r\n        ]\r\n\r\n        # Call LLM with context (implementation depends on chosen LLM)\r\n        return self.query_llm(messages)\n"})}),"\n",(0,s.jsx)(n.h2,{id:"structured-output-for-robot-commands",children:"Structured Output for Robot Commands"}),"\n",(0,s.jsx)(n.h3,{id:"command-extraction-and-validation",children:"Command Extraction and Validation"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"import json\r\nimport re\r\n\r\nclass CommandExtractionNode(Node):\r\n    def __init__(self):\r\n        super().__init__('command_extraction_node')\r\n\r\n        # Service for command extraction\r\n        self.command_service = self.create_service(\r\n            LLMQuery,\r\n            'extract_commands',\r\n            self.handle_command_extraction\r\n        )\r\n\r\n        # Publisher for extracted commands\r\n        self.command_pub = self.create_publisher(\r\n            String,  # Would typically be a custom command message\r\n            'robot_commands',\r\n            10\r\n        )\r\n\r\n        # Define supported command types\r\n        self.supported_commands = {\r\n            'navigation': ['go to', 'move to', 'navigate to', 'walk to'],\r\n            'manipulation': ['pick up', 'grasp', 'take', 'put down', 'place'],\r\n            'interaction': ['greet', 'wave', 'nod', 'shake hands'],\r\n            'information': ['what is', 'where is', 'find', 'show me']\r\n        }\r\n\r\n    def handle_command_extraction(self, request, response):\r\n        \"\"\"Extract structured commands from LLM response\"\"\"\r\n        try:\r\n            # Get LLM response\r\n            llm_response = self.query_llm([{\r\n                'role': 'user',\r\n                'content': f'Extract structured commands from this request: {request.query}. '\r\n                          f'Respond in JSON format with command type and parameters.'\r\n            }])\r\n\r\n            # Parse structured response\r\n            structured_commands = self.parse_structured_response(llm_response)\r\n\r\n            # Validate commands\r\n            valid_commands = self.validate_commands(structured_commands)\r\n\r\n            # Publish valid commands\r\n            for command in valid_commands:\r\n                cmd_msg = String()\r\n                cmd_msg.data = json.dumps(command)\r\n                self.command_pub.publish(cmd_msg)\r\n\r\n            response.success = True\r\n            response.response = json.dumps(valid_commands)\r\n\r\n        except Exception as e:\r\n            self.get_logger().error(f'Command extraction failed: {e}')\r\n            response.success = False\r\n            response.response = f'Error extracting commands: {e}'\r\n\r\n        return response\r\n\r\n    def parse_structured_response(self, llm_response):\r\n        \"\"\"Parse LLM response to extract structured commands\"\"\"\r\n        try:\r\n            # Try to parse as JSON first\r\n            return json.loads(llm_response)\r\n        except json.JSONDecodeError:\r\n            # If not JSON, try to extract using regex or other methods\r\n            return self.extract_commands_regex(llm_response)\r\n\r\n    def extract_commands_regex(self, text):\r\n        \"\"\"Extract commands using regex patterns\"\"\"\r\n        commands = []\r\n\r\n        # Navigation commands\r\n        nav_pattern = r'(?:go to|move to|navigate to|walk to) (.+)'\r\n        nav_matches = re.findall(nav_pattern, text, re.IGNORECASE)\r\n        for match in nav_matches:\r\n            commands.append({\r\n                'type': 'navigation',\r\n                'target': match.strip(),\r\n                'parameters': {}\r\n            })\r\n\r\n        # Manipulation commands\r\n        manip_pattern = r'(?:pick up|grasp|take) (.+)'\r\n        manip_matches = re.findall(manip_pattern, text, re.IGNORECASE)\r\n        for match in manip_matches:\r\n            commands.append({\r\n                'type': 'manipulation',\r\n                'target': match.strip(),\r\n                'parameters': {}\r\n            })\r\n\r\n        return commands\r\n\r\n    def validate_commands(self, commands):\r\n        \"\"\"Validate extracted commands for robot feasibility\"\"\"\r\n        valid_commands = []\r\n\r\n        for command in commands:\r\n            if self.is_command_valid(command):\r\n                valid_commands.append(command)\r\n\r\n        return valid_commands\r\n\r\n    def is_command_valid(self, command):\r\n        \"\"\"Check if a command is valid for the robot\"\"\"\r\n        # Check if command type is supported\r\n        if command.get('type') not in self.supported_commands:\r\n            return False\r\n\r\n        # Check if target is specified\r\n        if not command.get('target'):\r\n            return False\r\n\r\n        # Additional validation logic can be added here\r\n        return True\n"})}),"\n",(0,s.jsx)(n.h2,{id:"performance-optimization",children:"Performance Optimization"}),"\n",(0,s.jsx)(n.h3,{id:"caching-and-batching",children:"Caching and Batching"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'import functools\r\nimport time\r\nfrom collections import OrderedDict\r\n\r\nclass OptimizedLLMNode(Node):\r\n    def __init__(self):\r\n        super().__init__(\'optimized_llm_node\')\r\n\r\n        # Initialize cache\r\n        self.response_cache = OrderedDict()\r\n        self.cache_size = 50\r\n        self.cache_timeout = 300  # 5 minutes\r\n\r\n        # Rate limiting\r\n        self.request_times = []\r\n        self.max_requests_per_minute = 10\r\n\r\n    @functools.lru_cache(maxsize=128)\r\n    def cached_llm_query(self, query_hash):\r\n        """Cached LLM query to avoid repeated requests"""\r\n        # This would call the actual LLM with the original query\r\n        # Implementation depends on the specific LLM being used\r\n        pass\r\n\r\n    def should_rate_limit(self):\r\n        """Check if request should be rate limited"""\r\n        current_time = time.time()\r\n\r\n        # Remove old requests\r\n        self.request_times = [\r\n            req_time for req_time in self.request_times\r\n            if current_time - req_time < 60\r\n        ]\r\n\r\n        # Check if we\'re over the limit\r\n        if len(self.request_times) >= self.max_requests_per_minute:\r\n            return True\r\n\r\n        # Add current request\r\n        self.request_times.append(current_time)\r\n        return False\r\n\r\n    def query_with_caching(self, query, context=None):\r\n        """Query LLM with caching and rate limiting"""\r\n        # Create cache key\r\n        cache_key = f"{query}_{context or \'\'}" if context else query\r\n\r\n        # Check cache first\r\n        if cache_key in self.response_cache:\r\n            cached_response, timestamp = self.response_cache[cache_key]\r\n            if time.time() - timestamp < self.cache_timeout:\r\n                return cached_response\r\n\r\n        # Check rate limit\r\n        if self.should_rate_limit():\r\n            return "Please wait - processing too many requests"\r\n\r\n        # Call LLM\r\n        response = self.call_llm(query, context)\r\n\r\n        # Cache the response\r\n        self.cache_response(cache_key, response)\r\n\r\n        return response\r\n\r\n    def cache_response(self, key, response):\r\n        """Cache LLM response"""\r\n        if len(self.response_cache) >= self.cache_size:\r\n            # Remove oldest entry\r\n            self.response_cache.popitem(last=False)\r\n\r\n        self.response_cache[key] = (response, time.time())\n'})}),"\n",(0,s.jsx)(n.h2,{id:"safety-and-security-considerations",children:"Safety and Security Considerations"}),"\n",(0,s.jsx)(n.h3,{id:"content-filtering",children:"Content Filtering"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"class SafeLLMNode(Node):\r\n    def __init__(self):\r\n        super().__init__('safe_llm_node')\r\n\r\n        # Initialize safety filters\r\n        self.initialize_safety_filters()\r\n\r\n    def initialize_safety_filters(self):\r\n        \"\"\"Initialize content safety filters\"\"\"\r\n        self.blocked_keywords = [\r\n            'harm', 'injure', 'damage', 'unsafe', 'dangerous',\r\n            'break', 'destroy', 'hurt', 'kill', 'harmful'\r\n        ]\r\n\r\n        # Context-specific safety rules\r\n        self.safety_rules = [\r\n            # Rule: Don't allow commands that could harm humans\r\n            {\r\n                'pattern': r'(?:hurt|harm|injure|attack|hit|slap|push) (?:me|him|her|them|person|people|human)',\r\n                'response': \"I cannot perform actions that might harm humans. Is there something else I can help with?\"\r\n            }\r\n        ]\r\n\r\n    def process_safe_response(self, llm_response, original_request):\r\n        \"\"\"Process LLM response for safety\"\"\"\r\n        # Check for safety violations\r\n        if self.contains_blocked_content(llm_response):\r\n            return self.get_safe_alternative(original_request)\r\n\r\n        # Apply safety rules\r\n        for rule in self.safety_rules:\r\n            if re.search(rule['pattern'], llm_response, re.IGNORECASE):\r\n                return rule['response']\r\n\r\n        return llm_response\r\n\r\n    def contains_blocked_content(self, text):\r\n        \"\"\"Check if text contains blocked content\"\"\"\r\n        text_lower = text.lower()\r\n        return any(keyword in text_lower for keyword in self.blocked_keywords)\r\n\r\n    def get_safe_alternative(self, original_request):\r\n        \"\"\"Get a safe alternative response\"\"\"\r\n        safe_response = self.query_llm([{\r\n            'role': 'user',\r\n            'content': f'{original_request} - but respond safely and helpfully instead'\r\n        }])\r\n\r\n        return self.process_safe_response(safe_response, original_request)\n"})}),"\n",(0,s.jsx)(n.h2,{id:"integration-with-vision-and-action-systems",children:"Integration with Vision and Action Systems"}),"\n",(0,s.jsx)(n.h3,{id:"multi-modal-prompt-engineering",children:"Multi-Modal Prompt Engineering"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'class MultiModalLLMNode(Node):\r\n    def __init__(self):\r\n        super().__init__(\'multi_modal_llm_node\')\r\n\r\n        # Subscribe to vision and sensor data\r\n        self.vision_sub = self.create_subscription(\r\n            String,\r\n            \'vision_description\',\r\n            self.vision_callback,\r\n            10\r\n        )\r\n\r\n        self.sensors_sub = self.create_subscription(\r\n            String,\r\n            \'sensor_fusion\',\r\n            self.sensors_callback,\r\n            10\r\n        )\r\n\r\n        # Store multi-modal context\r\n        self.vision_data = {}\r\n        self.sensor_data = {}\r\n\r\n    def vision_callback(self, msg):\r\n        """Update vision context"""\r\n        try:\r\n            self.vision_data = json.loads(msg.data)\r\n        except json.JSONDecodeError:\r\n            self.vision_data = {\'description\': msg.data}\r\n\r\n    def sensors_callback(self, msg):\r\n        """Update sensor context"""\r\n        try:\r\n            self.sensor_data = json.loads(msg.data)\r\n        except json.JSONDecodeError:\r\n            self.sensor_data = {\'data\': msg.data}\r\n\r\n    def create_multimodal_prompt(self, user_request):\r\n        """Create prompt combining vision, sensor, and language inputs"""\r\n        prompt_parts = []\r\n\r\n        # Add user request\r\n        prompt_parts.append(f"User Request: {user_request}")\r\n\r\n        # Add vision context\r\n        if self.vision_data:\r\n            prompt_parts.append(f"Visual Context: {self.vision_data}")\r\n\r\n        # Add sensor context\r\n        if self.sensor_data:\r\n            prompt_parts.append(f"Sensor Context: {self.sensor_data}")\r\n\r\n        # Add robot state\r\n        robot_state = self.get_robot_state()\r\n        prompt_parts.append(f"Robot State: {robot_state}")\r\n\r\n        # Add capabilities\r\n        capabilities = self.get_robot_capabilities()\r\n        prompt_parts.append(f"Robot Capabilities: {capabilities}")\r\n\r\n        return "\\n".join(prompt_parts)\r\n\r\n    def get_robot_state(self):\r\n        """Get current robot state"""\r\n        # This would typically come from robot state publisher\r\n        return {\r\n            \'battery_level\': 85,\r\n            \'location\': \'kitchen\',\r\n            \'current_task\': \'idle\',\r\n            \'gripper_status\': \'open\'\r\n        }\n'})}),"\n",(0,s.jsx)(n.h2,{id:"troubleshooting-common-issues",children:"Troubleshooting Common Issues"}),"\n",(0,s.jsx)(n.h3,{id:"api-connection-problems",children:"API Connection Problems"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Issue"}),": LLM API calls failing due to connection issues."]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Solutions"}),":"]}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"Check API key validity and permissions"}),"\n",(0,s.jsx)(n.li,{children:"Verify network connectivity"}),"\n",(0,s.jsx)(n.li,{children:"Implement retry logic with exponential backoff"}),"\n",(0,s.jsx)(n.li,{children:"Use local fallback models when cloud services are unavailable"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"performance-issues",children:"Performance Issues"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Issue"}),": High latency in LLM responses affecting real-time interaction."]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Solutions"}),":"]}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"Use smaller, faster models for real-time responses"}),"\n",(0,s.jsx)(n.li,{children:"Implement response caching for common queries"}),"\n",(0,s.jsx)(n.li,{children:"Use streaming responses when supported"}),"\n",(0,s.jsx)(n.li,{children:"Preload frequently used prompts"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"context-window-limitations",children:"Context Window Limitations"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Issue"}),": Long conversations exceeding context window limits."]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Solutions"}),":"]}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"Implement conversation summarization"}),"\n",(0,s.jsx)(n.li,{children:"Use external memory systems"}),"\n",(0,s.jsx)(n.li,{children:"Implement sliding window context management"}),"\n",(0,s.jsx)(n.li,{children:"Compress context when necessary"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"best-practices",children:"Best Practices"}),"\n",(0,s.jsx)(n.h3,{id:"system-design",children:"System Design"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Modular Architecture"}),": Keep LLM integration separate for easy replacement"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Fallback Mechanisms"}),": Provide alternative responses when LLM fails"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Error Handling"}),": Implement comprehensive error handling and logging"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Performance Monitoring"}),": Track response times and success rates"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"privacy-and-ethics",children:"Privacy and Ethics"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Data Minimization"}),": Only send necessary information to LLMs"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Local Processing"}),": Use local models when privacy is critical"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Consent"}),": Obtain user consent for LLM interactions"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Transparency"}),": Inform users when LLMs are being used"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,s.jsxs)(n.p,{children:["Continue to ",(0,s.jsx)(n.a,{href:"/humanoid-robotics-book/vla-integration/cognitive-planning",children:"Cognitive Planning"})," to learn how to use LLMs for high-level reasoning and task planning in humanoid robots."]})]})}function m(e={}){const{wrapper:n}={...(0,t.RP)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}}}]);