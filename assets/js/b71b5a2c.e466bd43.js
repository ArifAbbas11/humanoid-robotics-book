"use strict";(globalThis.webpackChunkhumanoid_robotics_book=globalThis.webpackChunkhumanoid_robotics_book||[]).push([[4608],{5278:(n,e,r)=>{r.r(e),r.d(e,{assets:()=>c,contentTitle:()=>o,default:()=>m,frontMatter:()=>s,metadata:()=>a,toc:()=>l});var t=r(4848),i=r(8453);const s={},o="VLA Capstone Project: Intelligent Humanoid Assistant",a={id:"vla-integration/capstone-project",title:"VLA Capstone Project: Intelligent Humanoid Assistant",description:"Overview",source:"@site/docs/vla-integration/capstone-project.md",sourceDirName:"vla-integration",slug:"/vla-integration/capstone-project",permalink:"/humanoid-robotics-book/vla-integration/capstone-project",draft:!1,unlisted:!1,editUrl:"https://github.com/ArifAbbas11/humanoid-robotics-book/tree/main/docs/vla-integration/capstone-project.md",tags:[],version:"current",frontMatter:{},sidebar:"bookSidebar",previous:{title:"Voice-to-Action Mapping",permalink:"/humanoid-robotics-book/vla-integration/voice-to-action"},next:{title:"Troubleshooting VLA Integration",permalink:"/humanoid-robotics-book/vla-integration/troubleshooting"}},c={},l=[{value:"Overview",id:"overview",level:2},{value:"Project Objectives",id:"project-objectives",level:2},{value:"Project Requirements",id:"project-requirements",level:2},{value:"Hardware Requirements",id:"hardware-requirements",level:3},{value:"Software Requirements",id:"software-requirements",level:3},{value:"System Architecture",id:"system-architecture",level:2},{value:"High-Level Architecture",id:"high-level-architecture",level:3},{value:"Component Integration",id:"component-integration",level:3},{value:"Implementation Phase 1: Voice Recognition and Language Understanding",id:"implementation-phase-1-voice-recognition-and-language-understanding",level:2},{value:"Voice Recognition Integration",id:"voice-recognition-integration",level:3},{value:"Language Understanding System",id:"language-understanding-system",level:3},{value:"Implementation Phase 2: Vision and Multi-Modal Integration",id:"implementation-phase-2-vision-and-multi-modal-integration",level:2},{value:"Vision Processing System",id:"vision-processing-system",level:3},{value:"Multi-Modal Fusion Node",id:"multi-modal-fusion-node",level:3},{value:"Implementation Phase 3: Cognitive Planning and Action Execution",id:"implementation-phase-3-cognitive-planning-and-action-execution",level:2},{value:"Cognitive Planning Node",id:"cognitive-planning-node",level:3},{value:"Action Execution Node",id:"action-execution-node",level:3},{value:"Implementation Phase 4: System Integration and Testing",id:"implementation-phase-4-system-integration-and-testing",level:2},{value:"Main Launch File",id:"main-launch-file",level:3},{value:"Testing and Validation",id:"testing-and-validation",level:2},{value:"Unit Testing",id:"unit-testing",level:3},{value:"Integration Testing",id:"integration-testing",level:3},{value:"Performance Evaluation",id:"performance-evaluation",level:2},{value:"Metrics and Evaluation",id:"metrics-and-evaluation",level:3},{value:"Troubleshooting and Optimization",id:"troubleshooting-and-optimization",level:2},{value:"Common Issues and Solutions",id:"common-issues-and-solutions",level:3},{value:"Deployment and Documentation",id:"deployment-and-documentation",level:2},{value:"System Documentation",id:"system-documentation",level:3},{value:"Presentation and Demonstration",id:"presentation-and-demonstration",level:3},{value:"Next Steps",id:"next-steps",level:2}];function d(n){const e={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,i.RP)(),...n.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(e.h1,{id:"vla-capstone-project-intelligent-humanoid-assistant",children:"VLA Capstone Project: Intelligent Humanoid Assistant"}),"\n",(0,t.jsx)(e.h2,{id:"overview",children:"Overview"}),"\n",(0,t.jsx)(e.p,{children:"This capstone project integrates all Vision-Language-Action (VLA) concepts learned throughout the module to create an intelligent humanoid assistant capable of understanding natural language commands, perceiving its environment, and executing complex tasks. This comprehensive project demonstrates the complete VLA pipeline in a practical, real-world scenario."}),"\n",(0,t.jsx)(e.h2,{id:"project-objectives",children:"Project Objectives"}),"\n",(0,t.jsx)(e.p,{children:"By completing this capstone project, you will:"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsx)(e.li,{children:"Integrate voice recognition, language understanding, and action execution"}),"\n",(0,t.jsx)(e.li,{children:"Implement multi-modal perception combining vision and other sensors"}),"\n",(0,t.jsx)(e.li,{children:"Create a cognitive planning system for task decomposition"}),"\n",(0,t.jsx)(e.li,{children:"Build a complete VLA pipeline for humanoid robot control"}),"\n",(0,t.jsx)(e.li,{children:"Test and validate the integrated system in simulation and/or real hardware"}),"\n",(0,t.jsx)(e.li,{children:"Document and present your implementation and results"}),"\n"]}),"\n",(0,t.jsx)(e.h2,{id:"project-requirements",children:"Project Requirements"}),"\n",(0,t.jsx)(e.h3,{id:"hardware-requirements",children:"Hardware Requirements"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Humanoid Robot"}),": Physical robot or simulation environment (Gazebo/Isaac Sim)"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Sensors"}),": RGB-D camera, IMU, joint encoders, force/torque sensors"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Computing"}),": NVIDIA GPU for deep learning models (recommended)"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Audio"}),": Microphone for voice input, speaker for feedback"]}),"\n"]}),"\n",(0,t.jsx)(e.h3,{id:"software-requirements",children:"Software Requirements"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"ROS 2 Humble"}),": Robot Operating System for communication"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Isaac ROS"}),": GPU-accelerated perception and navigation"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Large Language Model"}),": OpenAI GPT, Claude, or open-source alternative"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Computer Vision"}),": Object detection, pose estimation, SLAM"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Development Environment"}),": Python, CUDA, Docker"]}),"\n"]}),"\n",(0,t.jsx)(e.h2,{id:"system-architecture",children:"System Architecture"}),"\n",(0,t.jsx)(e.h3,{id:"high-level-architecture",children:"High-Level Architecture"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-mermaid",children:"graph TD\r\n    A[User Voice Command] --\x3e B[Voice Recognition]\r\n    B --\x3e C[Language Understanding]\r\n    C --\x3e D[Cognitive Planning]\r\n    D --\x3e E[Multi-Modal Fusion]\r\n    E --\x3e F[Action Execution]\r\n    F --\x3e G[Robot Hardware]\r\n    G --\x3e H[Sensor Feedback]\r\n    H --\x3e E\r\n    G --\x3e I[User Feedback]\n"})}),"\n",(0,t.jsx)(e.h3,{id:"component-integration",children:"Component Integration"}),"\n",(0,t.jsx)(e.p,{children:"The system consists of several integrated components:"}),"\n",(0,t.jsxs)(e.ol,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Voice Processing Pipeline"}),": Speech-to-text conversion"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Language Understanding Module"}),": Command interpretation"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Vision System"}),": Environmental perception and object detection"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Cognitive Planner"}),": Task decomposition and planning"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Action Executor"}),": Robot control and execution"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Feedback System"}),": User communication and status updates"]}),"\n"]}),"\n",(0,t.jsx)(e.h2,{id:"implementation-phase-1-voice-recognition-and-language-understanding",children:"Implementation Phase 1: Voice Recognition and Language Understanding"}),"\n",(0,t.jsx)(e.h3,{id:"voice-recognition-integration",children:"Voice Recognition Integration"}),"\n",(0,t.jsx)(e.p,{children:"Create a comprehensive voice recognition system:"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:"# voice_recognition_node.py\r\nimport rclpy\r\nfrom rclpy.node import Node\r\nfrom std_msgs.msg import String\r\nfrom audio_common_msgs.msg import AudioData\r\nimport speech_recognition as sr\r\nimport threading\r\nimport queue\r\nimport vosk\r\nimport json\r\n\r\nclass VoiceRecognitionNode(Node):\r\n    def __init__(self):\r\n        super().__init__('voice_recognition_node')\r\n\r\n        # Publisher for recognized text\r\n        self.text_pub = self.create_publisher(String, 'recognized_text', 10)\r\n\r\n        # Subscriber for audio data\r\n        self.audio_sub = self.create_subscription(\r\n            AudioData,\r\n            'audio_input',\r\n            self.audio_callback,\r\n            10\r\n        )\r\n\r\n        # Initialize Vosk model for offline recognition\r\n        try:\r\n            self.model = vosk.Model(lang=\"en-us\")\r\n            self.rec = vosk.KaldiRecognizer(self.model, 16000)\r\n            self.get_logger().info('Vosk model loaded successfully')\r\n        except Exception as e:\r\n            self.get_logger().error(f'Failed to load Vosk model: {e}')\r\n            raise\r\n\r\n        # Voice activity detection\r\n        self.is_listening = True\r\n        self.command_queue = queue.Queue()\r\n\r\n        self.get_logger().info('Voice Recognition Node initialized')\r\n\r\n    def audio_callback(self, msg):\r\n        \"\"\"Process incoming audio data\"\"\"\r\n        try:\r\n            # Process audio chunk with Vosk\r\n            if self.rec.AcceptWaveform(msg.data):\r\n                result = self.rec.Result()\r\n                result_dict = json.loads(result)\r\n\r\n                if 'text' in result_dict and result_dict['text'].strip():\r\n                    # Publish recognized text\r\n                    text_msg = String()\r\n                    text_msg.data = result_dict['text'].strip()\r\n                    self.text_pub.publish(text_msg)\r\n                    self.get_logger().info(f'Recognized: {result_dict[\"text\"]}')\r\n\r\n        except Exception as e:\r\n            self.get_logger().error(f'Error in voice recognition: {e}')\n"})}),"\n",(0,t.jsx)(e.h3,{id:"language-understanding-system",children:"Language Understanding System"}),"\n",(0,t.jsx)(e.p,{children:"Implement natural language understanding:"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:"# language_understanding_node.py\r\nimport rclpy\r\nfrom rclpy.node import Node\r\nfrom std_msgs.msg import String\r\nfrom vla_msgs.msg import ParsedCommand\r\nimport spacy\r\nimport openai\r\nimport json\r\n\r\nclass LanguageUnderstandingNode(Node):\r\n    def __init__(self):\r\n        super().__init__('language_understanding_node')\r\n\r\n        # Initialize NLP model\r\n        try:\r\n            self.nlp = spacy.load(\"en_core_web_sm\")\r\n        except OSError:\r\n            self.get_logger().warn(\"spaCy model not found. Install with: python -m spacy download en_core_web_sm\")\r\n            self.nlp = None\r\n\r\n        # Subscriber for recognized text\r\n        self.text_sub = self.create_subscription(\r\n            String,\r\n            'recognized_text',\r\n            self.text_callback,\r\n            10\r\n        )\r\n\r\n        # Publisher for parsed commands\r\n        self.command_pub = self.create_publisher(ParsedCommand, 'parsed_command', 10)\r\n\r\n        # Initialize LLM client\r\n        self.llm_client = None  # Initialize with your chosen LLM\r\n\r\n        self.get_logger().info('Language Understanding Node initialized')\r\n\r\n    def text_callback(self, msg):\r\n        \"\"\"Process recognized text into structured commands\"\"\"\r\n        try:\r\n            # Parse the command using NLP\r\n            parsed_result = self.parse_command(msg.data)\r\n\r\n            # Create and publish parsed command\r\n            command_msg = ParsedCommand()\r\n            command_msg.header.stamp = self.get_clock().now().to_msg()\r\n            command_msg.original_text = msg.data\r\n            command_msg.intent = parsed_result.get('intent', 'unknown')\r\n            command_msg.action_type = parsed_result.get('action_type', 'unknown')\r\n            command_msg.parameters = json.dumps(parsed_result.get('parameters', {}))\r\n            command_msg.confidence = parsed_result.get('confidence', 0.0)\r\n\r\n            self.command_pub.publish(command_msg)\r\n            self.get_logger().info(f'Parsed command: {parsed_result}')\r\n\r\n        except Exception as e:\r\n            self.get_logger().error(f'Error parsing command: {e}')\r\n\r\n    def parse_command(self, text):\r\n        \"\"\"Parse natural language command into structured format\"\"\"\r\n        if self.nlp:\r\n            # Use spaCy for NLP processing\r\n            doc = self.nlp(text)\r\n\r\n            # Extract entities and intent\r\n            entities = [(ent.text, ent.label_) for ent in doc.ents]\r\n            intent = self.classify_intent(doc)\r\n\r\n            # Determine action type\r\n            action_type = self.extract_action_type(doc)\r\n\r\n            return {\r\n                'intent': intent,\r\n                'action_type': action_type,\r\n                'entities': entities,\r\n                'confidence': 0.8,  # Simplified confidence\r\n                'parameters': self.extract_parameters(doc, entities)\r\n            }\r\n\r\n        # Fallback simple parsing\r\n        return self.simple_parse(text)\r\n\r\n    def classify_intent(self, doc):\r\n        \"\"\"Classify the intent of the command\"\"\"\r\n        # Simple keyword-based classification\r\n        text = doc.text.lower()\r\n\r\n        if any(word in text for word in ['go', 'move', 'navigate', 'walk']):\r\n            return 'navigation'\r\n        elif any(word in text for word in ['pick', 'grasp', 'take', 'get']):\r\n            return 'manipulation'\r\n        elif any(word in text for word in ['greet', 'hello', 'wave']):\r\n            return 'interaction'\r\n        else:\r\n            return 'unknown'\r\n\r\n    def extract_action_type(self, doc):\r\n        \"\"\"Extract specific action type\"\"\"\r\n        for token in doc:\r\n            if token.pos_ == \"VERB\":\r\n                return token.lemma_\r\n        return 'unknown'\r\n\r\n    def extract_parameters(self, doc, entities):\r\n        \"\"\"Extract parameters from command\"\"\"\r\n        params = {}\r\n\r\n        # Extract object from entities\r\n        for ent_text, ent_label in entities:\r\n            if ent_label in ['OBJECT', 'PRODUCT', 'PERSON']:\r\n                params['target'] = ent_text\r\n\r\n        # Extract location from entities\r\n        for ent_text, ent_label in entities:\r\n            if ent_label in ['GPE', 'LOC', 'FAC']:\r\n                params['location'] = ent_text\r\n\r\n        return params\r\n\r\n    def simple_parse(self, text):\r\n        \"\"\"Simple fallback parsing\"\"\"\r\n        text_lower = text.lower()\r\n        params = {}\r\n\r\n        # Simple parameter extraction\r\n        if 'the' in text_lower:\r\n            parts = text_lower.split('the')\r\n            if len(parts) > 1:\r\n                params['target'] = parts[1].strip().split()[0]\r\n\r\n        return {\r\n            'intent': self.classify_intent_simple(text_lower),\r\n            'action_type': 'unknown',\r\n            'entities': [],\r\n            'confidence': 0.5,\r\n            'parameters': params\r\n        }\r\n\r\n    def classify_intent_simple(self, text_lower):\r\n        \"\"\"Simple intent classification\"\"\"\r\n        if any(word in text_lower for word in ['go', 'move', 'walk']):\r\n            return 'navigation'\r\n        elif any(word in text_lower for word in ['pick', 'grasp', 'take']):\r\n            return 'manipulation'\r\n        else:\r\n            return 'unknown'\n"})}),"\n",(0,t.jsx)(e.h2,{id:"implementation-phase-2-vision-and-multi-modal-integration",children:"Implementation Phase 2: Vision and Multi-Modal Integration"}),"\n",(0,t.jsx)(e.h3,{id:"vision-processing-system",children:"Vision Processing System"}),"\n",(0,t.jsx)(e.p,{children:"Create a comprehensive vision system:"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:"# vision_processing_node.py\r\nimport rclpy\r\nfrom rclpy.node import Node\r\nfrom sensor_msgs.msg import Image, CameraInfo\r\nfrom vision_msgs.msg import Detection2DArray, ObjectHypothesisWithPose\r\nfrom geometry_msgs.msg import Point\r\nfrom cv_bridge import CvBridge\r\nimport cv2\r\nimport numpy as np\r\nimport torch\r\nfrom torchvision import transforms\r\nfrom ultralytics import YOLO\r\n\r\nclass VisionProcessingNode(Node):\r\n    def __init__(self):\r\n        super().__init__('vision_processing_node')\r\n\r\n        # Initialize CV bridge\r\n        self.bridge = CvBridge()\r\n\r\n        # Load object detection model\r\n        self.detector = YOLO('yolov8n.pt')  # or your preferred model\r\n\r\n        # Subscribers\r\n        self.image_sub = self.create_subscription(\r\n            Image,\r\n            'camera/image_raw',\r\n            self.image_callback,\r\n            10\r\n        )\r\n\r\n        self.camera_info_sub = self.create_subscription(\r\n            CameraInfo,\r\n            'camera/camera_info',\r\n            self.camera_info_callback,\r\n            10\r\n        )\r\n\r\n        # Publishers\r\n        self.detection_pub = self.create_publisher(\r\n            Detection2DArray,\r\n            'object_detections',\r\n            10\r\n        )\r\n\r\n        self.visualization_pub = self.create_publisher(\r\n            Image,\r\n            'vision_visualization',\r\n            10\r\n        )\r\n\r\n        # Internal state\r\n        self.camera_matrix = None\r\n        self.distortion_coeffs = None\r\n\r\n        self.get_logger().info('Vision Processing Node initialized')\r\n\r\n    def camera_info_callback(self, msg):\r\n        \"\"\"Process camera calibration information\"\"\"\r\n        self.camera_matrix = np.array(msg.k).reshape(3, 3)\r\n        self.distortion_coeffs = np.array(msg.d)\r\n\r\n    def image_callback(self, msg):\r\n        \"\"\"Process incoming camera image\"\"\"\r\n        try:\r\n            # Convert ROS image to OpenCV\r\n            cv_image = self.bridge.imgmsg_to_cv2(msg, \"bgr8\")\r\n\r\n            # Perform object detection\r\n            results = self.detector(cv_image)\r\n\r\n            # Process detections\r\n            detections = self.process_detections(results, cv_image)\r\n\r\n            # Create and publish detection message\r\n            detection_msg = self.create_detection_message(detections, msg.header)\r\n            self.detection_pub.publish(detection_msg)\r\n\r\n            # Create visualization\r\n            vis_image = self.visualize_detections(cv_image, results)\r\n            vis_msg = self.bridge.cv2_to_imgmsg(vis_image, \"bgr8\")\r\n            vis_msg.header = msg.header\r\n            self.visualization_pub.publish(vis_msg)\r\n\r\n        except Exception as e:\r\n            self.get_logger().error(f'Error processing image: {e}')\r\n\r\n    def process_detections(self, results, image):\r\n        \"\"\"Process YOLO detection results\"\"\"\r\n        detections = []\r\n\r\n        for result in results:\r\n            for box in result.boxes:\r\n                # Get bounding box coordinates\r\n                x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()\r\n                confidence = float(box.conf[0])\r\n                class_id = int(box.cls[0])\r\n\r\n                # Get class name\r\n                class_name = self.detector.names[class_id]\r\n\r\n                # Calculate center point\r\n                center_x = int((x1 + x2) / 2)\r\n                center_y = int((y1 + y2) / 2)\r\n\r\n                detection = {\r\n                    'class_name': class_name,\r\n                    'confidence': confidence,\r\n                    'bbox': [int(x1), int(y1), int(x2-x1), int(y2-y1)],\r\n                    'center': [center_x, center_y],\r\n                    'class_id': class_id\r\n                }\r\n\r\n                detections.append(detection)\r\n\r\n        return detections\r\n\r\n    def create_detection_message(self, detections, header):\r\n        \"\"\"Create ROS detection message\"\"\"\r\n        detection_array = Detection2DArray()\r\n        detection_array.header = header\r\n\r\n        for detection in detections:\r\n            if detection['confidence'] > 0.5:  # Confidence threshold\r\n                detection_msg = Detection2D()\r\n                detection_msg.header = header\r\n\r\n                # Set bounding box\r\n                detection_msg.bbox.size_x = detection['bbox'][2]\r\n                detection_msg.bbox.size_y = detection['bbox'][3]\r\n\r\n                # Set center point\r\n                detection_msg.bbox.center.x = detection['center'][0]\r\n                detection_msg.bbox.center.y = detection['center'][1]\r\n\r\n                # Set hypothesis\r\n                hypothesis = ObjectHypothesisWithPose()\r\n                hypothesis.hypothesis.class_id = detection['class_name']\r\n                hypothesis.hypothesis.score = detection['confidence']\r\n                detection_msg.results.append(hypothesis)\r\n\r\n                detection_array.detections.append(detection_msg)\r\n\r\n        return detection_array\r\n\r\n    def visualize_detections(self, image, results):\r\n        \"\"\"Create visualization of detections\"\"\"\r\n        annotated_image = image.copy()\r\n\r\n        for result in results:\r\n            for box in result.boxes:\r\n                x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()\r\n                confidence = float(box.conf[0])\r\n                class_id = int(box.cls[0])\r\n                class_name = self.detector.names[class_id]\r\n\r\n                # Draw bounding box\r\n                cv2.rectangle(\r\n                    annotated_image,\r\n                    (int(x1), int(y1)),\r\n                    (int(x2), int(y2)),\r\n                    (0, 255, 0),\r\n                    2\r\n                )\r\n\r\n                # Draw label\r\n                label = f\"{class_name}: {confidence:.2f}\"\r\n                cv2.putText(\r\n                    annotated_image,\r\n                    label,\r\n                    (int(x1), int(y1) - 10),\r\n                    cv2.FONT_HERSHEY_SIMPLEX,\r\n                    0.5,\r\n                    (0, 255, 0),\r\n                    2\r\n                )\r\n\r\n        return annotated_image\n"})}),"\n",(0,t.jsx)(e.h3,{id:"multi-modal-fusion-node",children:"Multi-Modal Fusion Node"}),"\n",(0,t.jsx)(e.p,{children:"Integrate vision and language understanding:"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:"# multi_modal_fusion_node.py\r\nimport rclpy\r\nfrom rclpy.node import Node\r\nfrom std_msgs.msg import String\r\nfrom vision_msgs.msg import Detection2DArray\r\nfrom vla_msgs.msg import ParsedCommand, FusedPerception\r\nfrom geometry_msgs.msg import Point\r\nimport json\r\n\r\nclass MultiModalFusionNode(Node):\r\n    def __init__(self):\r\n        super().__init__('multi_modal_fusion_node')\r\n\r\n        # Subscribers\r\n        self.command_sub = self.create_subscription(\r\n            ParsedCommand,\r\n            'parsed_command',\r\n            self.command_callback,\r\n            10\r\n        )\r\n\r\n        self.detection_sub = self.create_subscription(\r\n            Detection2DArray,\r\n            'object_detections',\r\n            self.detection_callback,\r\n            10\r\n        )\r\n\r\n        # Publisher\r\n        self.fusion_pub = self.create_publisher(FusedPerception, 'fused_perception', 10)\r\n\r\n        # Internal state\r\n        self.latest_command = None\r\n        self.latest_detections = []\r\n        self.knowledge_base = {}  # Store object locations and properties\r\n\r\n        self.get_logger().info('Multi-Modal Fusion Node initialized')\r\n\r\n    def command_callback(self, msg):\r\n        \"\"\"Process parsed command\"\"\"\r\n        self.latest_command = msg\r\n        self.process_fusion()\r\n\r\n    def detection_callback(self, msg):\r\n        \"\"\"Process object detections\"\"\"\r\n        self.latest_detections = msg.detections\r\n        self.update_knowledge_base(msg.detections)\r\n        self.process_fusion()\r\n\r\n    def process_fusion(self):\r\n        \"\"\"Process fusion when both command and detections are available\"\"\"\r\n        if self.latest_command and self.latest_detections:\r\n            try:\r\n                # Fuse command and detections\r\n                fused_result = self.fuse_command_and_detections(\r\n                    self.latest_command,\r\n                    self.latest_detections\r\n                )\r\n\r\n                # Publish fused result\r\n                fusion_msg = self.create_fusion_message(fused_result)\r\n                self.fusion_pub.publish(fusion_msg)\r\n\r\n                # Clear processed data\r\n                self.latest_command = None\r\n\r\n            except Exception as e:\r\n                self.get_logger().error(f'Error in fusion processing: {e}')\r\n\r\n    def fuse_command_and_detections(self, command, detections):\r\n        \"\"\"Fuse command understanding with object detections\"\"\"\r\n        fused_result = {\r\n            'command': {\r\n                'intent': command.intent,\r\n                'action_type': command.action_type,\r\n                'parameters': json.loads(command.parameters),\r\n                'original_text': command.original_text\r\n            },\r\n            'environment': {\r\n                'objects': self.extract_object_info(detections),\r\n                'relevant_objects': []\r\n            },\r\n            'grounded_command': None\r\n        }\r\n\r\n        # Ground the command in the environment\r\n        if command.intent in ['manipulation', 'navigation']:\r\n            relevant_objects = self.find_relevant_objects(\r\n                fused_result['command']['parameters'],\r\n                fused_result['environment']['objects']\r\n            )\r\n            fused_result['environment']['relevant_objects'] = relevant_objects\r\n\r\n        # Create grounded command\r\n        fused_result['grounded_command'] = self.ground_command(\r\n            fused_result['command'],\r\n            fused_result['environment']['relevant_objects']\r\n        )\r\n\r\n        return fused_result\r\n\r\n    def extract_object_info(self, detections):\r\n        \"\"\"Extract object information from detections\"\"\"\r\n        objects = []\r\n\r\n        for detection in detections:\r\n            obj_info = {\r\n                'class_name': detection.results[0].hypothesis.class_id if detection.results else 'unknown',\r\n                'confidence': detection.results[0].hypothesis.score if detection.results else 0.0,\r\n                'bbox_center_x': detection.bbox.center.x,\r\n                'bbox_center_y': detection.bbox.center.y,\r\n                'bbox_width': detection.bbox.size_x,\r\n                'bbox_height': detection.bbox.size_y\r\n            }\r\n            objects.append(obj_info)\r\n\r\n        return objects\r\n\r\n    def find_relevant_objects(self, command_params, objects):\r\n        \"\"\"Find objects relevant to the command\"\"\"\r\n        relevant_objects = []\r\n\r\n        target_object = command_params.get('target', '').lower()\r\n\r\n        for obj in objects:\r\n            if target_object in obj['class_name'].lower() or obj['confidence'] > 0.8:\r\n                relevant_objects.append(obj)\r\n\r\n        return relevant_objects\r\n\r\n    def ground_command(self, command, relevant_objects):\r\n        \"\"\"Ground the command in the environment\"\"\"\r\n        if not relevant_objects:\r\n            return command  # Return original command if no objects found\r\n\r\n        # Update command with grounded information\r\n        grounded_command = command.copy()\r\n        grounded_command['grounded_objects'] = relevant_objects\r\n\r\n        # If command involves a specific object, ground it\r\n        if command['parameters'].get('target'):\r\n            for obj in relevant_objects:\r\n                if command['parameters']['target'].lower() in obj['class_name'].lower():\r\n                    grounded_command['target_object'] = obj\r\n                    break\r\n\r\n        return grounded_command\r\n\r\n    def update_knowledge_base(self, detections):\r\n        \"\"\"Update knowledge base with current object information\"\"\"\r\n        for detection in detections:\r\n            if detection.results:\r\n                class_name = detection.results[0].hypothesis.class_id\r\n                confidence = detection.results[0].hypothesis.score\r\n\r\n                if confidence > 0.5:  # Confidence threshold\r\n                    self.knowledge_base[class_name] = {\r\n                        'last_seen': self.get_clock().now().to_msg(),\r\n                        'location': {\r\n                            'x': detection.bbox.center.x,\r\n                            'y': detection.bbox.center.y\r\n                        },\r\n                        'confidence': confidence\r\n                    }\r\n\r\n    def create_fusion_message(self, fused_result):\r\n        \"\"\"Create ROS message from fused result\"\"\"\r\n        fusion_msg = FusedPerception()\r\n        fusion_msg.header.stamp = self.get_clock().now().to_msg()\r\n        fusion_msg.header.frame_id = \"map\"\r\n\r\n        # Set command information\r\n        fusion_msg.command_intent = fused_result['command']['intent']\r\n        fusion_msg.command_action_type = fused_result['command']['action_type']\r\n        fusion_msg.command_original_text = fused_result['command']['original_text']\r\n\r\n        # Set environment information\r\n        for obj in fused_result['environment']['objects']:\r\n            # Create object info message (simplified)\r\n            pass\r\n\r\n        # Set grounded command\r\n        if fused_result['grounded_command']:\r\n            fusion_msg.has_grounded_command = True\r\n\r\n        return fusion_msg\n"})}),"\n",(0,t.jsx)(e.h2,{id:"implementation-phase-3-cognitive-planning-and-action-execution",children:"Implementation Phase 3: Cognitive Planning and Action Execution"}),"\n",(0,t.jsx)(e.h3,{id:"cognitive-planning-node",children:"Cognitive Planning Node"}),"\n",(0,t.jsx)(e.p,{children:"Implement high-level reasoning and task planning:"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:"# cognitive_planning_node.py\r\nimport rclpy\r\nfrom rclpy.node import Node\r\nfrom vla_msgs.msg import FusedPerception, ActionPlan\r\nfrom vla_msgs.srv import PlanAction\r\nfrom geometry_msgs.msg import Pose\r\nimport json\r\n\r\nclass CognitivePlanningNode(Node):\r\n    def __init__(self):\r\n        super().__init__('cognitive_planning_node')\r\n\r\n        # Subscriber for fused perception\r\n        self.fusion_sub = self.create_subscription(\r\n            FusedPerception,\r\n            'fused_perception',\r\n            self.fusion_callback,\r\n            10\r\n        )\r\n\r\n        # Publisher for action plans\r\n        self.plan_pub = self.create_publisher(ActionPlan, 'action_plan', 10)\r\n\r\n        # Service for planning requests\r\n        self.plan_service = self.create_service(\r\n            PlanAction,\r\n            'plan_action',\r\n            self.plan_action_callback\r\n        )\r\n\r\n        # Initialize planner components\r\n        self.knowledge_base = KnowledgeBase()\r\n        self.task_decomposer = TaskDecomposer()\r\n        self.action_validator = ActionValidator()\r\n\r\n        self.get_logger().info('Cognitive Planning Node initialized')\r\n\r\n    def fusion_callback(self, msg):\r\n        \"\"\"Process fused perception data to generate plans\"\"\"\r\n        try:\r\n            # Extract command and environment information\r\n            command_info = {\r\n                'intent': msg.command_intent,\r\n                'action_type': msg.command_action_type,\r\n                'original_text': msg.command_original_text\r\n            }\r\n\r\n            # Generate plan based on command and environment\r\n            plan = self.generate_plan(command_info)\r\n\r\n            if plan:\r\n                # Publish the plan\r\n                plan_msg = self.create_plan_message(plan)\r\n                self.plan_pub.publish(plan_msg)\r\n\r\n        except Exception as e:\r\n            self.get_logger().error(f'Error in fusion processing: {e}')\r\n\r\n    def generate_plan(self, command_info):\r\n        \"\"\"Generate action plan from command information\"\"\"\r\n        try:\r\n            # Decompose high-level command into subtasks\r\n            subtasks = self.task_decomposer.decompose_task(\r\n                command_info['intent'],\r\n                command_info['action_type']\r\n            )\r\n\r\n            # Create detailed action plan\r\n            plan = {\r\n                'original_command': command_info['original_text'],\r\n                'intent': command_info['intent'],\r\n                'action_sequence': [],\r\n                'estimated_duration': 0.0,\r\n                'confidence': 0.9\r\n            }\r\n\r\n            for i, subtask in enumerate(subtasks):\r\n                action = self.create_action_for_subtask(subtask, i)\r\n                if action:\r\n                    plan['action_sequence'].append(action)\r\n\r\n            # Validate the plan\r\n            if self.action_validator.validate_plan(plan):\r\n                return plan\r\n            else:\r\n                self.get_logger().warn('Generated plan failed validation')\r\n                return None\r\n\r\n        except Exception as e:\r\n            self.get_logger().error(f'Error generating plan: {e}')\r\n            return None\r\n\r\n    def create_action_for_subtask(self, subtask, step_index):\r\n        \"\"\"Create specific action for a subtask\"\"\"\r\n        action = {\r\n            'step': step_index,\r\n            'action_type': subtask['type'],\r\n            'action_name': subtask['name'],\r\n            'parameters': subtask.get('parameters', {}),\r\n            'description': subtask['description'],\r\n            'required_resources': subtask.get('resources', []),\r\n            'estimated_duration': subtask.get('duration', 1.0)\r\n        }\r\n\r\n        return action\r\n\r\n    def plan_action_callback(self, request, response):\r\n        \"\"\"Service callback for explicit planning requests\"\"\"\r\n        try:\r\n            # Parse request\r\n            command_info = {\r\n                'intent': request.intent,\r\n                'action_type': request.action_type,\r\n                'original_text': request.command_text\r\n            }\r\n\r\n            # Generate plan\r\n            plan = self.generate_plan(command_info)\r\n\r\n            if plan:\r\n                response.success = True\r\n                response.plan = self.create_plan_message(plan)\r\n                response.message = \"Plan generated successfully\"\r\n            else:\r\n                response.success = False\r\n                response.message = \"Failed to generate plan\"\r\n\r\n        except Exception as e:\r\n            self.get_logger().error(f'Plan service error: {e}')\r\n            response.success = False\r\n            response.message = f\"Error: {e}\"\r\n\r\n        return response\r\n\r\n    def create_plan_message(self, plan):\r\n        \"\"\"Create ROS message from plan dictionary\"\"\"\r\n        plan_msg = ActionPlan()\r\n        plan_msg.header.stamp = self.get_clock().now().to_msg()\r\n        plan_msg.header.frame_id = \"map\"\r\n\r\n        plan_msg.original_command = plan['original_command']\r\n        plan_msg.intent = plan['intent']\r\n        plan_msg.confidence = plan['confidence']\r\n        plan_msg.estimated_duration = plan['estimated_duration']\r\n\r\n        for action in plan['action_sequence']:\r\n            # Convert action dictionary to ROS message\r\n            action_msg = self.create_action_message(action)\r\n            plan_msg.actions.append(action_msg)\r\n\r\n        return plan_msg\r\n\r\n    def create_action_message(self, action_dict):\r\n        \"\"\"Create action message from dictionary\"\"\"\r\n        from vla_msgs.msg import ActionStep\r\n        action_msg = ActionStep()\r\n        action_msg.step_number = action_dict['step']\r\n        action_msg.action_type = action_dict['action_type']\r\n        action_msg.action_name = action_dict['action_name']\r\n        action_msg.parameters = json.dumps(action_dict['parameters'])\r\n        action_msg.description = action_dict['description']\r\n        action_msg.estimated_duration = action_dict['estimated_duration']\r\n        return action_msg\r\n\r\nclass TaskDecomposer:\r\n    \"\"\"Decompose high-level tasks into executable subtasks\"\"\"\r\n    def __init__(self):\r\n        self.task_templates = {\r\n            'navigation': [\r\n                {\r\n                    'type': 'navigation',\r\n                    'name': 'navigate_to',\r\n                    'description': 'Navigate to specified location',\r\n                    'parameters': ['destination'],\r\n                    'resources': ['navigation_system']\r\n                }\r\n            ],\r\n            'manipulation': [\r\n                {\r\n                    'type': 'navigation',\r\n                    'name': 'navigate_to_object',\r\n                    'description': 'Navigate close to target object',\r\n                    'parameters': ['object_location'],\r\n                    'resources': ['navigation_system']\r\n                },\r\n                {\r\n                    'type': 'manipulation',\r\n                    'name': 'grasp_object',\r\n                    'description': 'Grasp the target object',\r\n                    'parameters': ['object_pose'],\r\n                    'resources': ['manipulator_arm', 'gripper']\r\n                }\r\n            ],\r\n            'transport': [\r\n                {\r\n                    'type': 'navigation',\r\n                    'name': 'navigate_to_object',\r\n                    'description': 'Navigate to object location',\r\n                    'parameters': ['object_location'],\r\n                    'resources': ['navigation_system']\r\n                },\r\n                {\r\n                    'type': 'manipulation',\r\n                    'name': 'grasp_object',\r\n                    'description': 'Grasp the object',\r\n                    'parameters': ['object_pose'],\r\n                    'resources': ['manipulator_arm', 'gripper']\r\n                },\r\n                {\r\n                    'type': 'navigation',\r\n                    'name': 'navigate_to_destination',\r\n                    'description': 'Navigate to destination',\r\n                    'parameters': ['destination'],\r\n                    'resources': ['navigation_system']\r\n                },\r\n                {\r\n                    'type': 'manipulation',\r\n                    'name': 'place_object',\r\n                    'description': 'Place object at destination',\r\n                    'parameters': ['destination_pose'],\r\n                    'resources': ['manipulator_arm', 'gripper']\r\n                }\r\n            ]\r\n        }\r\n\r\n    def decompose_task(self, intent, action_type):\r\n        \"\"\"Decompose task based on intent and action type\"\"\"\r\n        if intent in self.task_templates:\r\n            return self.task_templates[intent]\r\n        elif action_type in self.task_templates:\r\n            return self.task_templates[action_type]\r\n        else:\r\n            # Default to simple navigation\r\n            return self.task_templates.get('navigation', [])\r\n\r\nclass KnowledgeBase:\r\n    \"\"\"Maintain knowledge about the world and robot capabilities\"\"\"\r\n    def __init__(self):\r\n        self.locations = {}\r\n        self.objects = {}\r\n        self.robot_capabilities = {\r\n            'navigation': True,\r\n            'manipulation': True,\r\n            'interaction': True,\r\n            'perception': True\r\n        }\r\n\r\nclass ActionValidator:\r\n    \"\"\"Validate action plans for feasibility and safety\"\"\"\r\n    def __init__(self):\r\n        pass\r\n\r\n    def validate_plan(self, plan):\r\n        \"\"\"Validate that a plan is feasible and safe\"\"\"\r\n        # Check if all required resources are available\r\n        for action in plan['action_sequence']:\r\n            if not self.resources_available(action):\r\n                return False\r\n\r\n        # Check for safety constraints\r\n        if not self.check_safety_constraints(plan):\r\n            return False\r\n\r\n        # Check for logical consistency\r\n        if not self.check_logical_consistency(plan):\r\n            return False\r\n\r\n        return True\r\n\r\n    def resources_available(self, action):\r\n        \"\"\"Check if required resources are available\"\"\"\r\n        # Implementation would check robot state and resource availability\r\n        return True\r\n\r\n    def check_safety_constraints(self, plan):\r\n        \"\"\"Check if plan violates safety constraints\"\"\"\r\n        # Implementation would check for collision risks, etc.\r\n        return True\r\n\r\n    def check_logical_consistency(self, plan):\r\n        \"\"\"Check if plan steps are logically consistent\"\"\"\r\n        # Implementation would check action dependencies, etc.\r\n        return True\n"})}),"\n",(0,t.jsx)(e.h3,{id:"action-execution-node",children:"Action Execution Node"}),"\n",(0,t.jsx)(e.p,{children:"Execute the planned actions on the robot:"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:"# action_execution_node.py\r\nimport rclpy\r\nfrom rclpy.node import Node\r\nfrom vla_msgs.msg import ActionPlan, ActionResult\r\nfrom geometry_msgs.msg import Pose\r\nfrom std_msgs.msg import String\r\nimport time\r\n\r\nclass ActionExecutionNode(Node):\r\n    def __init__(self):\r\n        super().__init__('action_execution_node')\r\n\r\n        # Subscriber for action plans\r\n        self.plan_sub = self.create_subscription(\r\n            ActionPlan,\r\n            'action_plan',\r\n            self.plan_callback,\r\n            10\r\n        )\r\n\r\n        # Publisher for action results\r\n        self.result_pub = self.create_publisher(ActionResult, 'action_result', 10)\r\n\r\n        # Publisher for user feedback\r\n        self.feedback_pub = self.create_publisher(String, 'user_feedback', 10)\r\n\r\n        # Initialize action executors\r\n        self.navigation_executor = NavigationExecutor(self)\r\n        self.manipulation_executor = ManipulationExecutor(self)\r\n        self.interaction_executor = InteractionExecutor(self)\r\n\r\n        self.is_executing = False\r\n        self.current_plan = None\r\n\r\n        self.get_logger().info('Action Execution Node initialized')\r\n\r\n    def plan_callback(self, msg):\r\n        \"\"\"Execute incoming action plan\"\"\"\r\n        if self.is_executing:\r\n            self.get_logger().warn('Currently executing plan, rejecting new plan')\r\n            return\r\n\r\n        self.get_logger().info(f'Executing plan with {len(msg.actions)} actions')\r\n        self.is_executing = True\r\n        self.current_plan = msg\r\n\r\n        try:\r\n            # Execute each action in sequence\r\n            results = []\r\n            for i, action in enumerate(msg.actions):\r\n                self.get_logger().info(f'Executing action {i+1}/{len(msg.actions)}: {action.action_name}')\r\n\r\n                # Execute the action\r\n                result = self.execute_action(action)\r\n\r\n                # Publish result\r\n                result_msg = self.create_result_message(result, i)\r\n                self.result_pub.publish(result_msg)\r\n\r\n                results.append(result)\r\n\r\n                # Check if execution should continue\r\n                if not result.success:\r\n                    self.get_logger().error(f'Action {i+1} failed: {result.message}')\r\n                    break\r\n\r\n            # Publish execution summary\r\n            summary = self.create_execution_summary(results)\r\n            summary_msg = String()\r\n            summary_msg.data = summary\r\n            self.feedback_pub.publish(summary_msg)\r\n\r\n        except Exception as e:\r\n            self.get_logger().error(f'Error executing plan: {e}')\r\n            error_msg = String()\r\n            error_msg.data = f'Execution error: {e}'\r\n            self.feedback_pub.publish(error_msg)\r\n\r\n        finally:\r\n            self.is_executing = False\r\n            self.current_plan = None\r\n\r\n    def execute_action(self, action):\r\n        \"\"\"Execute a single action based on its type\"\"\"\r\n        try:\r\n            if action.action_type == 'navigation':\r\n                return self.navigation_executor.execute(action)\r\n            elif action.action_type == 'manipulation':\r\n                return self.manipulation_executor.execute(action)\r\n            elif action.action_type == 'interaction':\r\n                return self.interaction_executor.execute(action)\r\n            else:\r\n                return {\r\n                    'success': False,\r\n                    'message': f'Unknown action type: {action.action_type}',\r\n                    'action_name': action.action_name\r\n                }\r\n\r\n        except Exception as e:\r\n            return {\r\n                'success': False,\r\n                'message': f'Execution error: {e}',\r\n                'action_name': action.action_name\r\n            }\r\n\r\n    def create_result_message(self, result, action_index):\r\n        \"\"\"Create ROS result message from result dictionary\"\"\"\r\n        result_msg = ActionResult()\r\n        result_msg.header.stamp = self.get_clock().now().to_msg()\r\n        result_msg.action_index = action_index\r\n        result_msg.success = result['success']\r\n        result_msg.message = result['message']\r\n        result_msg.action_name = result['action_name']\r\n        return result_msg\r\n\r\n    def create_execution_summary(self, results):\r\n        \"\"\"Create summary of execution results\"\"\"\r\n        successful = sum(1 for r in results if r['success'])\r\n        total = len(results)\r\n\r\n        status = \"completed successfully\" if successful == total else \"partially completed\"\r\n\r\n        return f\"Plan {status}: {successful}/{total} actions successful\"\r\n\r\nclass NavigationExecutor:\r\n    \"\"\"Execute navigation-related actions\"\"\"\r\n    def __init__(self, node):\r\n        self.node = node\r\n        # Initialize navigation interface (Navigation2, etc.)\r\n\r\n    def execute(self, action):\r\n        \"\"\"Execute navigation action\"\"\"\r\n        try:\r\n            # Extract destination from parameters\r\n            import json\r\n            params = json.loads(action.parameters)\r\n            destination = params.get('destination', params.get('destination_location'))\r\n\r\n            if not destination:\r\n                return {\r\n                    'success': False,\r\n                    'message': 'No destination specified',\r\n                    'action_name': action.action_name\r\n                }\r\n\r\n            # Execute navigation (this would interface with Navigation2)\r\n            self.node.get_logger().info(f'Navigating to: {destination}')\r\n\r\n            # Simulate navigation execution\r\n            time.sleep(2)  # Simulate navigation time\r\n\r\n            return {\r\n                'success': True,\r\n                'message': f'Navigated to {destination}',\r\n                'action_name': action.action_name\r\n            }\r\n\r\n        except Exception as e:\r\n            return {\r\n                'success': False,\r\n                'message': f'Navigation error: {e}',\r\n                'action_name': action.action_name\r\n            }\r\n\r\nclass ManipulationExecutor:\r\n    \"\"\"Execute manipulation-related actions\"\"\"\r\n    def __init__(self, node):\r\n        self.node = node\r\n        # Initialize manipulation interface\r\n\r\n    def execute(self, action):\r\n        \"\"\"Execute manipulation action\"\"\"\r\n        try:\r\n            import json\r\n            params = json.loads(action.parameters)\r\n\r\n            if action.action_name == 'grasp_object':\r\n                object_info = params.get('object_pose') or params.get('object')\r\n                self.node.get_logger().info(f'Grasping object: {object_info}')\r\n\r\n                # Simulate grasping\r\n                time.sleep(1.5)\r\n\r\n                return {\r\n                    'success': True,\r\n                    'message': f'Grasped object',\r\n                    'action_name': action.action_name\r\n                }\r\n\r\n            elif action.action_name == 'place_object':\r\n                location = params.get('destination_pose') or params.get('location')\r\n                self.node.get_logger().info(f'Placing object at: {location}')\r\n\r\n                # Simulate placing\r\n                time.sleep(1.5)\r\n\r\n                return {\r\n                    'success': True,\r\n                    'message': f'Placed object',\r\n                    'action_name': action.action_name\r\n                }\r\n            else:\r\n                return {\r\n                    'success': False,\r\n                    'message': f'Unknown manipulation action: {action.action_name}',\r\n                    'action_name': action.action_name\r\n                }\r\n\r\n        except Exception as e:\r\n            return {\r\n                'success': False,\r\n                'message': f'Manipulation error: {e}',\r\n                'action_name': action.action_name\r\n            }\r\n\r\nclass InteractionExecutor:\r\n    \"\"\"Execute interaction-related actions\"\"\"\r\n    def __init__(self, node):\r\n        self.node = node\r\n        # Initialize interaction interface\r\n\r\n    def execute(self, action):\r\n        \"\"\"Execute interaction action\"\"\"\r\n        try:\r\n            self.node.get_logger().info(f'Performing interaction: {action.action_name}')\r\n\r\n            # Simulate interaction\r\n            time.sleep(1)\r\n\r\n            return {\r\n                'success': True,\r\n                'message': f'Performed interaction: {action.action_name}',\r\n                'action_name': action.action_name\r\n            }\r\n\r\n        except Exception as e:\r\n            return {\r\n                'success': False,\r\n                'message': f'Interaction error: {e}',\r\n                'action_name': action.action_name\r\n            }\n"})}),"\n",(0,t.jsx)(e.h2,{id:"implementation-phase-4-system-integration-and-testing",children:"Implementation Phase 4: System Integration and Testing"}),"\n",(0,t.jsx)(e.h3,{id:"main-launch-file",children:"Main Launch File"}),"\n",(0,t.jsx)(e.p,{children:"Create a launch file to bring up the complete system:"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:"# launch/vla_capstone.launch.py\r\nfrom launch import LaunchDescription\r\nfrom launch.actions import DeclareLaunchArgument, IncludeLaunchDescription\r\nfrom launch.launch_description_sources import PythonLaunchDescriptionSource\r\nfrom launch.substitutions import LaunchConfiguration, PathJoinSubstitution\r\nfrom launch_ros.actions import Node\r\nfrom launch_ros.substitutions import FindPackageShare\r\n\r\ndef generate_launch_description():\r\n    ld = LaunchDescription()\r\n\r\n    # Launch arguments\r\n    use_sim_time = LaunchConfiguration('use_sim_time', default='true')\r\n    robot_namespace = LaunchConfiguration('robot_namespace', default='humanoid_robot')\r\n\r\n    # Declare launch arguments\r\n    ld.add_action(DeclareLaunchArgument(\r\n        'use_sim_time',\r\n        default_value='true',\r\n        description='Use simulation (Gazebo) clock if true'\r\n    ))\r\n\r\n    ld.add_action(DeclareLaunchArgument(\r\n        'robot_namespace',\r\n        default_value='humanoid_robot',\r\n        description='Robot namespace for multi-robot systems'\r\n    ))\r\n\r\n    # Launch Gazebo simulation (if needed)\r\n    # This would include your humanoid robot model\r\n\r\n    # Voice recognition node\r\n    voice_recognition_node = Node(\r\n        package='vla_capstone',\r\n        executable='voice_recognition_node',\r\n        name='voice_recognition_node',\r\n        parameters=[{'use_sim_time': use_sim_time}],\r\n        output='screen'\r\n    )\r\n    ld.add_action(voice_recognition_node)\r\n\r\n    # Language understanding node\r\n    language_understanding_node = Node(\r\n        package='vla_capstone',\r\n        executable='language_understanding_node',\r\n        name='language_understanding_node',\r\n        parameters=[{'use_sim_time': use_sim_time}],\r\n        output='screen'\r\n    )\r\n    ld.add_action(language_understanding_node)\r\n\r\n    # Vision processing node\r\n    vision_processing_node = Node(\r\n        package='vla_capstone',\r\n        executable='vision_processing_node',\r\n        name='vision_processing_node',\r\n        parameters=[{'use_sim_time': use_sim_time}],\r\n        output='screen'\r\n    )\r\n    ld.add_action(vision_processing_node)\r\n\r\n    # Multi-modal fusion node\r\n    multi_modal_fusion_node = Node(\r\n        package='vla_capstone',\r\n        executable='multi_modal_fusion_node',\r\n        name='multi_modal_fusion_node',\r\n        parameters=[{'use_sim_time': use_sim_time}],\r\n        output='screen'\r\n    )\r\n    ld.add_action(multi_modal_fusion_node)\r\n\r\n    # Cognitive planning node\r\n    cognitive_planning_node = Node(\r\n        package='vla_capstone',\r\n        executable='cognitive_planning_node',\r\n        name='cognitive_planning_node',\r\n        parameters=[{'use_sim_time': use_sim_time}],\r\n        output='screen'\r\n    )\r\n    ld.add_action(cognitive_planning_node)\r\n\r\n    # Action execution node\r\n    action_execution_node = Node(\r\n        package='vla_capstone',\r\n        executable='action_execution_node',\r\n        name='action_execution_node',\r\n        parameters=[{'use_sim_time': use_sim_time}],\r\n        output='screen'\r\n    )\r\n    ld.add_action(action_execution_node)\r\n\r\n    # User feedback node\r\n    user_feedback_node = Node(\r\n        package='vla_capstone',\r\n        executable='user_feedback_node',\r\n        name='user_feedback_node',\r\n        parameters=[{'use_sim_time': use_sim_time}],\r\n        output='screen'\r\n    )\r\n    ld.add_action(user_feedback_node)\r\n\r\n    return ld\n"})}),"\n",(0,t.jsx)(e.h2,{id:"testing-and-validation",children:"Testing and Validation"}),"\n",(0,t.jsx)(e.h3,{id:"unit-testing",children:"Unit Testing"}),"\n",(0,t.jsx)(e.p,{children:"Create comprehensive tests for each component:"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:'# test_vla_capstone.py\r\nimport unittest\r\nimport rclpy\r\nfrom rclpy.node import Node\r\nfrom std_msgs.msg import String\r\nfrom sensor_msgs.msg import Image\r\nimport numpy as np\r\nimport cv2\r\n\r\nclass TestVoiceRecognition(unittest.TestCase):\r\n    def setUp(self):\r\n        rclpy.init()\r\n        self.node = Node(\'test_voice_recognition\')\r\n\r\n    def tearDown(self):\r\n        rclpy.shutdown()\r\n\r\n    def test_command_parsing(self):\r\n        """Test that commands are parsed correctly"""\r\n        from language_understanding_node import LanguageUnderstandingNode\r\n        parser = LanguageUnderstandingNode()\r\n\r\n        # Test navigation command\r\n        result = parser.simple_parse("go to the kitchen")\r\n        self.assertEqual(result[\'intent\'], \'navigation\')\r\n\r\n        # Test manipulation command\r\n        result = parser.simple_parse("pick up the red cup")\r\n        self.assertEqual(result[\'intent\'], \'manipulation\')\r\n\r\nclass TestVisionProcessing(unittest.TestCase):\r\n    def setUp(self):\r\n        rclpy.init()\r\n\r\n    def tearDown(self):\r\n        rclpy.shutdown()\r\n\r\n    def test_object_detection(self):\r\n        """Test that objects are detected correctly"""\r\n        from vision_processing_node import VisionProcessingNode\r\n        vision_node = VisionProcessingNode()\r\n\r\n        # Create a test image with a known object\r\n        test_image = np.zeros((480, 640, 3), dtype=np.uint8)\r\n        # Add a colored rectangle to simulate an object\r\n        cv2.rectangle(test_image, (100, 100), (200, 200), (0, 255, 0), -1)\r\n\r\n        # Convert to ROS Image message and process\r\n        # (Implementation would go here)\r\n\r\nif __name__ == \'__main__\':\r\n    unittest.main()\n'})}),"\n",(0,t.jsx)(e.h3,{id:"integration-testing",children:"Integration Testing"}),"\n",(0,t.jsx)(e.p,{children:"Test the complete system integration:"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:"# integration_test.py\r\nimport rclpy\r\nfrom rclpy.node import Node\r\nfrom std_msgs.msg import String\r\nfrom vla_msgs.msg import ActionPlan, ActionResult\r\nimport time\r\n\r\nclass VLASystemTester(Node):\r\n    def __init__(self):\r\n        super().__init__('vla_system_tester')\r\n\r\n        # Publishers for test commands\r\n        self.voice_pub = self.create_publisher(String, 'recognized_text', 10)\r\n\r\n        # Subscribers for results\r\n        self.plan_sub = self.create_subscription(\r\n            ActionPlan, 'action_plan', self.plan_callback, 10)\r\n        self.result_sub = self.create_subscription(\r\n            ActionResult, 'action_result', self.result_callback, 10)\r\n\r\n        self.received_plans = []\r\n        self.received_results = []\r\n\r\n    def plan_callback(self, msg):\r\n        self.received_plans.append(msg)\r\n        self.get_logger().info(f'Received plan with {len(msg.actions)} actions')\r\n\r\n    def result_callback(self, msg):\r\n        self.received_results.append(msg)\r\n        self.get_logger().info(f'Received result: {msg.message}')\r\n\r\n    def test_navigation_command(self):\r\n        \"\"\"Test a simple navigation command\"\"\"\r\n        test_command = String()\r\n        test_command.data = \"go to the kitchen\"\r\n\r\n        self.voice_pub.publish(test_command)\r\n        self.get_logger().info('Published navigation command')\r\n\r\n        # Wait for response\r\n        time.sleep(5)\r\n\r\n        # Check if plan was received\r\n        if self.received_plans:\r\n            plan = self.received_plans[0]\r\n            self.get_logger().info(f'Plan received with {len(plan.actions)} actions')\r\n            return True\r\n        else:\r\n            self.get_logger().error('No plan received')\r\n            return False\r\n\r\ndef main():\r\n    rclpy.init()\r\n    tester = VLASystemTester()\r\n\r\n    # Run tests\r\n    success = tester.test_navigation_command()\r\n\r\n    if success:\r\n        print(\"Integration test passed!\")\r\n    else:\r\n        print(\"Integration test failed!\")\r\n\r\n    rclpy.shutdown()\r\n\r\nif __name__ == '__main__':\r\n    main()\n"})}),"\n",(0,t.jsx)(e.h2,{id:"performance-evaluation",children:"Performance Evaluation"}),"\n",(0,t.jsx)(e.h3,{id:"metrics-and-evaluation",children:"Metrics and Evaluation"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:'# evaluation_metrics.py\r\nimport time\r\nimport numpy as np\r\n\r\nclass VLASystemEvaluator:\r\n    def __init__(self):\r\n        self.metrics = {\r\n            \'response_time\': [],\r\n            \'accuracy\': [],\r\n            \'success_rate\': [],\r\n            \'user_satisfaction\': []\r\n        }\r\n\r\n    def measure_response_time(self, command_time, response_time):\r\n        """Measure system response time"""\r\n        response_duration = response_time - command_time\r\n        self.metrics[\'response_time\'].append(response_duration)\r\n        return response_duration\r\n\r\n    def evaluate_accuracy(self, expected_action, executed_action):\r\n        """Evaluate action execution accuracy"""\r\n        # Compare expected vs executed actions\r\n        accuracy = 1.0 if expected_action == executed_action else 0.0\r\n        self.metrics[\'accuracy\'].append(accuracy)\r\n        return accuracy\r\n\r\n    def evaluate_success_rate(self, total_commands, successful_commands):\r\n        """Calculate success rate"""\r\n        success_rate = successful_commands / total_commands if total_commands > 0 else 0\r\n        self.metrics[\'success_rate\'].append(success_rate)\r\n        return success_rate\r\n\r\n    def calculate_average_metrics(self):\r\n        """Calculate average performance metrics"""\r\n        averages = {}\r\n        for metric, values in self.metrics.items():\r\n            if values:\r\n                averages[metric] = sum(values) / len(values)\r\n            else:\r\n                averages[metric] = 0.0\r\n        return averages\r\n\r\n    def generate_report(self):\r\n        """Generate performance evaluation report"""\r\n        averages = self.calculate_average_metrics()\r\n\r\n        report = f"""\r\n        VLA System Performance Report\r\n        =============================\r\n        Average Response Time: {averages[\'response_time\']:.2f}s\r\n        Average Accuracy: {averages[\'accuracy\']:.2f}\r\n        Average Success Rate: {averages[\'success_rate\']:.2f}\r\n        """\r\n\r\n        return report\n'})}),"\n",(0,t.jsx)(e.h2,{id:"troubleshooting-and-optimization",children:"Troubleshooting and Optimization"}),"\n",(0,t.jsx)(e.h3,{id:"common-issues-and-solutions",children:"Common Issues and Solutions"}),"\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Issue"}),": High latency in voice-to-action pipeline."]}),"\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Solutions"}),":"]}),"\n",(0,t.jsxs)(e.ol,{children:["\n",(0,t.jsx)(e.li,{children:"Optimize model inference with GPU acceleration"}),"\n",(0,t.jsx)(e.li,{children:"Implement caching for common commands"}),"\n",(0,t.jsx)(e.li,{children:"Use lightweight models for real-time processing"}),"\n",(0,t.jsx)(e.li,{children:"Optimize ROS 2 communication settings"}),"\n"]}),"\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Issue"}),": Misunderstood commands leading to incorrect actions."]}),"\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Solutions"}),":"]}),"\n",(0,t.jsxs)(e.ol,{children:["\n",(0,t.jsx)(e.li,{children:"Implement confidence thresholds for command acceptance"}),"\n",(0,t.jsx)(e.li,{children:"Add clarification requests for ambiguous commands"}),"\n",(0,t.jsx)(e.li,{children:"Improve language understanding with context"}),"\n",(0,t.jsx)(e.li,{children:"Use multiple verification steps before action execution"}),"\n"]}),"\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Issue"}),": Vision system fails in different lighting conditions."]}),"\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Solutions"}),":"]}),"\n",(0,t.jsxs)(e.ol,{children:["\n",(0,t.jsx)(e.li,{children:"Implement adaptive image preprocessing"}),"\n",(0,t.jsx)(e.li,{children:"Use multiple detection models for robustness"}),"\n",(0,t.jsx)(e.li,{children:"Add temporal consistency checks"}),"\n",(0,t.jsx)(e.li,{children:"Integrate with other sensors (LIDAR, depth cameras)"}),"\n"]}),"\n",(0,t.jsx)(e.h2,{id:"deployment-and-documentation",children:"Deployment and Documentation"}),"\n",(0,t.jsx)(e.h3,{id:"system-documentation",children:"System Documentation"}),"\n",(0,t.jsx)(e.p,{children:"Create comprehensive documentation for your VLA system:"}),"\n",(0,t.jsxs)(e.ol,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Architecture Documentation"}),": System design and component interactions"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"API Documentation"}),": Message types, services, and interfaces"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"User Manual"}),": How to operate and interact with the system"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Troubleshooting Guide"}),": Common issues and solutions"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Performance Benchmarks"}),": System capabilities and limitations"]}),"\n"]}),"\n",(0,t.jsx)(e.h3,{id:"presentation-and-demonstration",children:"Presentation and Demonstration"}),"\n",(0,t.jsx)(e.p,{children:"Prepare a demonstration of your complete VLA system:"}),"\n",(0,t.jsxs)(e.ol,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Live Demo"}),": Show the system responding to voice commands"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Technical Presentation"}),": Explain the architecture and implementation"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Performance Analysis"}),": Present evaluation results"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Future Improvements"}),": Discuss potential enhancements"]}),"\n"]}),"\n",(0,t.jsx)(e.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,t.jsxs)(e.p,{children:["Continue to ",(0,t.jsx)(e.a,{href:"/humanoid-robotics-book/vla-integration/troubleshooting",children:"Troubleshooting"})," to learn about common VLA system issues and their solutions, completing the VLA Integration module."]})]})}function m(n={}){const{wrapper:e}={...(0,i.RP)(),...n.components};return e?(0,t.jsx)(e,{...n,children:(0,t.jsx)(d,{...n})}):d(n)}},8453:(n,e,r)=>{r.d(e,{RP:()=>s});var t=r(6540);const i=t.createContext({});function s(n){const e=t.useContext(i);return t.useMemo(()=>"function"==typeof n?n(e):{...e,...n},[e,n])}}}]);