"use strict";(globalThis.webpackChunkhumanoid_robotics_book=globalThis.webpackChunkhumanoid_robotics_book||[]).push([[8197],{984:(n,r,e)=>{e.r(r),e.d(r,{assets:()=>s,contentTitle:()=>a,default:()=>d,frontMatter:()=>o,metadata:()=>l,toc:()=>c});var i=e(4848),t=e(8453);const o={},a="Mini-Project: Implementing a VLA System for a Humanoid Robot",l={id:"vla-integration/mini-project",title:"Mini-Project: Implementing a VLA System for a Humanoid Robot",description:"Overview",source:"@site/docs/vla-integration/mini-project.md",sourceDirName:"vla-integration",slug:"/vla-integration/mini-project",permalink:"/humanoid-robotics-book/vla-integration/mini-project",draft:!1,unlisted:!1,editUrl:"https://github.com/ArifAbbas11/humanoid-robotics-book/tree/main/docs/vla-integration/mini-project.md",tags:[],version:"current",frontMatter:{}},s={},c=[{value:"Overview",id:"overview",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"Step 1: Create the Robot Model with VLA Sensors",id:"step-1-create-the-robot-model-with-vla-sensors",level:2},{value:"Step 2: Create the VLA Integration Node",id:"step-2-create-the-vla-integration-node",level:2},{value:"Step 3: Create Supporting Configuration Files",id:"step-3-create-supporting-configuration-files",level:2},{value:"Step 4: Create the Launch File",id:"step-4-create-the-launch-file",level:2},{value:"Step 5: Create a Simple Voice Command Simulator",id:"step-5-create-a-simple-voice-command-simulator",level:2},{value:"Step 6: Build and Run the VLA System",id:"step-6-build-and-run-the-vla-system",level:2},{value:"Expected Results",id:"expected-results",level:2},{value:"Troubleshooting",id:"troubleshooting",level:2},{value:"Next Steps",id:"next-steps",level:2}];function m(n){const r={a:"a",code:"code",h1:"h1",h2:"h2",li:"li",ol:"ol",p:"p",pre:"pre",ul:"ul",...(0,t.RP)(),...n.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(r.h1,{id:"mini-project-implementing-a-vla-system-for-a-humanoid-robot",children:"Mini-Project: Implementing a VLA System for a Humanoid Robot"}),"\n",(0,i.jsx)(r.h2,{id:"overview",children:"Overview"}),"\n",(0,i.jsx)(r.p,{children:"In this mini-project, you'll implement a complete Vision-Language-Action (VLA) system for a humanoid robot that can understand natural language commands, perceive objects in its environment, and execute appropriate actions. You'll create a system that demonstrates the integration of vision, language, and action components."}),"\n",(0,i.jsx)(r.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,i.jsxs)(r.ul,{children:["\n",(0,i.jsx)(r.li,{children:"Completed previous VLA integration sections"}),"\n",(0,i.jsx)(r.li,{children:"ROS 2 Humble installed"}),"\n",(0,i.jsx)(r.li,{children:"Gazebo simulation environment"}),"\n",(0,i.jsx)(r.li,{children:"Basic understanding of computer vision and natural language processing"}),"\n"]}),"\n",(0,i.jsx)(r.h2,{id:"step-1-create-the-robot-model-with-vla-sensors",children:"Step 1: Create the Robot Model with VLA Sensors"}),"\n",(0,i.jsxs)(r.p,{children:["Create ",(0,i.jsx)(r.code,{children:"vla_humanoid.urdf.xacro"}),":"]}),"\n",(0,i.jsx)(r.pre,{children:(0,i.jsx)(r.code,{className:"language-xml",children:'<?xml version="1.0"?>\r\n<robot xmlns:xacro="http://www.ros.org/wiki/xacro" name="vla_humanoid">\r\n\r\n  \x3c!-- Base torso --\x3e\r\n  <link name="base_link">\r\n    <visual>\r\n      <geometry>\r\n        <box size="0.3 0.2 0.6"/>\r\n      </geometry>\r\n      <material name="blue">\r\n        <color rgba="0 0 1 0.8"/>\r\n      </material>\r\n    </visual>\r\n    <collision>\r\n      <geometry>\r\n        <box size="0.3 0.2 0.6"/>\r\n      </geometry>\r\n    </collision>\r\n    <inertial>\r\n      <mass value="50"/>\r\n      <inertia ixx="2.5" ixy="0" ixz="0" iyy="3.5" iyz="0" izz="1.5"/>\r\n    </inertial>\r\n  </link>\r\n\r\n  \x3c!-- Head with cameras --\x3e\r\n  <link name="head">\r\n    <visual>\r\n      <geometry>\r\n        <sphere radius="0.1"/>\r\n      </geometry>\r\n      <material name="white">\r\n        <color rgba="1 1 1 0.8"/>\r\n      </material>\r\n    </visual>\r\n    <collision>\r\n      <geometry>\r\n        <sphere radius="0.1"/>\r\n      </geometry>\r\n    </collision>\r\n    <inertial>\r\n      <mass value="2"/>\r\n      <inertia ixx="0.004" ixy="0" ixz="0" iyy="0.004" iyz="0" izz="0.004"/>\r\n    </inertial>\r\n  </link>\r\n\r\n  <joint name="neck_joint" type="revolute">\r\n    <parent link="base_link"/>\r\n    <child link="head"/>\r\n    <origin xyz="0 0 0.3" rpy="0 0 0"/>\r\n    <axis xyz="0 1 0"/>\r\n    <limit lower="-0.5" upper="0.5" effort="100" velocity="1"/>\r\n  </joint>\r\n\r\n  \x3c!-- RGB-D Camera --\x3e\r\n  <link name="camera_link">\r\n    <visual>\r\n      <geometry>\r\n        <box size="0.05 0.05 0.05"/>\r\n      </geometry>\r\n      <material name="black">\r\n        <color rgba="0 0 0 0.8"/>\r\n      </material>\r\n    </visual>\r\n    <collision>\r\n      <geometry>\r\n        <box size="0.05 0.05 0.05"/>\r\n      </geometry>\r\n    </collision>\r\n    <inertial>\r\n      <mass value="0.1"/>\r\n      <inertia ixx="0.0001" ixy="0" ixz="0" iyy="0.0001" iyz="0" izz="0.0001"/>\r\n    </inertial>\r\n  </link>\r\n\r\n  <joint name="camera_joint" type="fixed">\r\n    <parent link="head"/>\r\n    <child link="camera_link"/>\r\n    <origin xyz="0.05 0 0" rpy="0 0 0"/>\r\n  </joint>\r\n\r\n  \x3c!-- Stereo cameras for depth perception --\x3e\r\n  <link name="left_camera_link">\r\n    <visual>\r\n      <geometry>\r\n        <box size="0.02 0.02 0.02"/>\r\n      </geometry>\r\n    </visual>\r\n    <collision>\r\n      <geometry>\r\n        <box size="0.02 0.02 0.02"/>\r\n      </geometry>\r\n    </collision>\r\n    <inertial>\r\n      <mass value="0.05"/>\r\n      <inertia ixx="0.00005" ixy="0" ixz="0" iyy="0.00005" iyz="0" izz="0.00005"/>\r\n    </inertial>\r\n  </link>\r\n\r\n  <link name="right_camera_link">\r\n    <visual>\r\n      <geometry>\r\n        <box size="0.02 0.02 0.02"/>\r\n      </geometry>\r\n    </visual>\r\n    <collision>\r\n      <geometry>\r\n        <box size="0.02 0.02 0.02"/>\r\n      </geometry>\r\n    </collision>\r\n    <inertial>\r\n      <mass value="0.05"/>\r\n      <inertia ixx="0.00005" ixy="0" ixz="0" iyy="0.00005" iyz="0" izz="0.00005"/>\r\n    </inertial>\r\n  </link>\r\n\r\n  <joint name="left_camera_joint" type="fixed">\r\n    <parent link="head"/>\r\n    <child link="left_camera_link"/>\r\n    <origin xyz="0.06 0.03 0" rpy="0 0 0"/>\r\n  </joint>\r\n\r\n  <joint name="right_camera_joint" type="fixed">\r\n    <parent link="head"/>\r\n    <child link="right_camera_link"/>\r\n    <origin xyz="0.06 -0.03 0" rpy="0 0 0"/>\r\n  </joint>\r\n\r\n  \x3c!-- IMU for balance --\x3e\r\n  <link name="imu_link">\r\n    <visual>\r\n      <geometry>\r\n        <box size="0.02 0.02 0.02"/>\r\n      </geometry>\r\n    </visual>\r\n    <collision>\r\n      <geometry>\r\n        <box size="0.02 0.02 0.02"/>\r\n      </geometry>\r\n    </collision>\r\n    <inertial>\r\n      <mass value="0.05"/>\r\n      <inertia ixx="0.000001" ixy="0" ixz="0" iyy="0.000001" iyz="0" izz="0.000001"/>\r\n    </inertial>\r\n  </link>\r\n\r\n  <joint name="imu_joint" type="fixed">\r\n    <parent link="base_link"/>\r\n    <child link="imu_link"/>\r\n    <origin xyz="0 0 0" rpy="0 0 0"/>\r\n  </joint>\r\n\r\n  \x3c!-- Left arm --\x3e\r\n  <link name="left_shoulder">\r\n    <visual>\r\n      <geometry>\r\n        <box size="0.1 0.1 0.1"/>\r\n      </geometry>\r\n    </visual>\r\n    <collision>\r\n      <geometry>\r\n        <box size="0.1 0.1 0.1"/>\r\n      </geometry>\r\n    </collision>\r\n    <inertial>\r\n      <mass value="2"/>\r\n      <inertia ixx="0.005" ixy="0" ixz="0" iyy="0.005" iyz="0" izz="0.005"/>\r\n    </inertial>\r\n  </link>\r\n\r\n  <joint name="left_shoulder_joint" type="revolute">\r\n    <parent link="base_link"/>\r\n    <child link="left_shoulder"/>\r\n    <origin xyz="0.15 0.1 0.1" rpy="0 0 0"/>\r\n    <axis xyz="0 1 0"/>\r\n    <limit lower="-1.57" upper="1.57" effort="100" velocity="2"/>\r\n  </joint>\r\n\r\n  <link name="left_upper_arm">\r\n    <visual>\r\n      <geometry>\r\n        <box size="0.08 0.08 0.3"/>\r\n      </geometry>\r\n    </visual>\r\n    <collision>\r\n      <geometry>\r\n        <box size="0.08 0.08 0.3"/>\r\n      </geometry>\r\n    </collision>\r\n    <inertial>\r\n      <mass value="1.5"/>\r\n      <inertia ixx="0.02" ixy="0" ixz="0" iyy="0.02" iyz="0" izz="0.005"/>\r\n    </inertial>\r\n  </link>\r\n\r\n  <joint name="left_elbow_joint" type="revolute">\r\n    <parent link="left_shoulder"/>\r\n    <child link="left_upper_arm"/>\r\n    <origin xyz="0 0 -0.15" rpy="0 0 0"/>\r\n    <axis xyz="0 0 1"/>\r\n    <limit lower="-2.35" upper="0" effort="100" velocity="2"/>\r\n  </joint>\r\n\r\n  <link name="left_forearm">\r\n    <visual>\r\n      <geometry>\r\n        <box size="0.06 0.06 0.25"/>\r\n      </geometry>\r\n    </visual>\r\n    <collision>\r\n      <geometry>\r\n        <box size="0.06 0.06 0.25"/>\r\n      </geometry>\r\n    </collision>\r\n    <inertial>\r\n      <mass value="1"/>\r\n      <inertia ixx="0.01" ixy="0" ixz="0" iyy="0.01" iyz="0" izz="0.003"/>\r\n    </inertial>\r\n  </link>\r\n\r\n  <joint name="left_wrist_joint" type="revolute">\r\n    <parent link="left_upper_arm"/>\r\n    <child link="left_forearm"/>\r\n    <origin xyz="0 0 -0.25" rpy="0 0 0"/>\r\n    <axis xyz="0 1 0"/>\r\n    <limit lower="-1.57" upper="1.57" effort="50" velocity="2"/>\r\n  </joint>\r\n\r\n  <link name="left_hand">\r\n    <visual>\r\n      <geometry>\r\n        <box size="0.1 0.08 0.05"/>\r\n      </geometry>\r\n    </visual>\r\n    <collision>\r\n      <geometry>\r\n        <box size="0.1 0.08 0.05"/>\r\n      </geometry>\r\n    </collision>\r\n    <inertial>\r\n      <mass value="0.3"/>\r\n      <inertia ixx="0.001" ixy="0" ixz="0" iyy="0.001" iyz="0" izz="0.001"/>\r\n    </inertial>\r\n  </link>\r\n\r\n  <joint name="left_hand_joint" type="fixed">\r\n    <parent link="left_forearm"/>\r\n    <child link="left_hand"/>\r\n    <origin xyz="0 0 -0.05" rpy="0 0 0"/>\r\n  </joint>\r\n\r\n  \x3c!-- Right arm --\x3e\r\n  <link name="right_shoulder">\r\n    <visual>\r\n      <geometry>\r\n        <box size="0.1 0.1 0.1"/>\r\n      </geometry>\r\n    </visual>\r\n    <collision>\r\n      <geometry>\r\n        <box size="0.1 0.1 0.1"/>\r\n      </geometry>\r\n    </collision>\r\n    <inertial>\r\n      <mass value="2"/>\r\n      <inertia ixx="0.005" ixy="0" ixz="0" iyy="0.005" iyz="0" izz="0.005"/>\r\n    </inertial>\r\n  </link>\r\n\r\n  <joint name="right_shoulder_joint" type="revolute">\r\n    <parent link="base_link"/>\r\n    <child link="right_shoulder"/>\r\n    <origin xyz="0.15 -0.1 0.1" rpy="0 0 0"/>\r\n    <axis xyz="0 1 0"/>\r\n    <limit lower="-1.57" upper="1.57" effort="100" velocity="2"/>\r\n  </joint>\r\n\r\n  <link name="right_upper_arm">\r\n    <visual>\r\n      <geometry>\r\n        <box size="0.08 0.08 0.3"/>\r\n      </geometry>\r\n    </visual>\r\n    <collision>\r\n      <geometry>\r\n        <box size="0.08 0.08 0.3"/>\r\n      </geometry>\r\n    </collision>\r\n    <inertial>\r\n      <mass value="1.5"/>\r\n      <inertia ixx="0.02" ixy="0" ixz="0" iyy="0.02" iyz="0" izz="0.005"/>\r\n    </inertial>\r\n  </link>\r\n\r\n  <joint name="right_elbow_joint" type="revolute">\r\n    <parent link="right_shoulder"/>\r\n    <child link="right_upper_arm"/>\r\n    <origin xyz="0 0 -0.15" rpy="0 0 0"/>\r\n    <axis xyz="0 0 1"/>\r\n    <limit lower="-2.35" upper="0" effort="100" velocity="2"/>\r\n  </joint>\r\n\r\n  <link name="right_forearm">\r\n    <visual>\r\n      <geometry>\r\n        <box size="0.06 0.06 0.25"/>\r\n      </geometry>\r\n    </visual>\r\n    <collision>\r\n      <geometry>\r\n        <box size="0.06 0.06 0.25"/>\r\n      </geometry>\r\n    </collision>\r\n    <inertial>\r\n      <mass value="1"/>\r\n      <inertia ixx="0.01" ixy="0" ixz="0" iyy="0.01" iyz="0" izz="0.003"/>\r\n    </inertial>\r\n  </link>\r\n\r\n  <joint name="right_wrist_joint" type="revolute">\r\n    <parent link="right_upper_arm"/>\r\n    <child link="right_forearm"/>\r\n    <origin xyz="0 0 -0.25" rpy="0 0 0"/>\r\n    <axis xyz="0 1 0"/>\r\n    <limit lower="-1.57" upper="1.57" effort="50" velocity="2"/>\r\n  </joint>\r\n\r\n  <link name="right_hand">\r\n    <visual>\r\n      <geometry>\r\n        <box size="0.1 0.08 0.05"/>\r\n      </geometry>\r\n    </visual>\r\n    <collision>\r\n      <geometry>\r\n        <box size="0.1 0.08 0.05"/>\r\n      </geometry>\r\n    </collision>\r\n    <inertial>\r\n      <mass value="0.3"/>\r\n      <inertia ixx="0.001" ixy="0" ixz="0" iyy="0.001" iyz="0" izz="0.001"/>\r\n    </inertial>\r\n  </link>\r\n\r\n  <joint name="right_hand_joint" type="fixed">\r\n    <parent link="right_forearm"/>\r\n    <child link="right_hand"/>\r\n    <origin xyz="0 0 -0.05" rpy="0 0 0"/>\r\n  </joint>\r\n\r\n  \x3c!-- Legs for stability --\x3e\r\n  <link name="left_hip">\r\n    <visual>\r\n      <geometry>\r\n        <box size="0.1 0.1 0.1"/>\r\n      </geometry>\r\n    </visual>\r\n    <collision>\r\n      <geometry>\r\n        <box size="0.1 0.1 0.1"/>\r\n      </geometry>\r\n    </collision>\r\n    <inertial>\r\n      <mass value="5"/>\r\n      <inertia ixx="0.02" ixy="0" ixz="0" iyy="0.02" iyz="0" izz="0.02"/>\r\n    </inertial>\r\n  </link>\r\n\r\n  <joint name="left_hip_joint" type="fixed">\r\n    <parent link="base_link"/>\r\n    <child link="left_hip"/>\r\n    <origin xyz="0 0.1 -0.3" rpy="0 0 0"/>\r\n  </joint>\r\n\r\n  <link name="left_knee">\r\n    <visual>\r\n      <geometry>\r\n        <box size="0.1 0.1 0.3"/>\r\n      </geometry>\r\n    </visual>\r\n    <collision>\r\n      <geometry>\r\n        <box size="0.1 0.1 0.3"/>\r\n      </geometry>\r\n    </collision>\r\n    <inertial>\r\n      <mass value="3"/>\r\n      <inertia ixx="0.05" ixy="0" ixz="0" iyy="0.05" iyz="0" izz="0.01"/>\r\n    </inertial>\r\n  </link>\r\n\r\n  <joint name="left_knee_joint" type="fixed">\r\n    <parent link="left_hip"/>\r\n    <child link="left_knee"/>\r\n    <origin xyz="0 0 -0.2" rpy="0 0 0"/>\r\n  </joint>\r\n\r\n  <link name="left_ankle">\r\n    <visual>\r\n      <geometry>\r\n        <box size="0.2 0.1 0.05"/>\r\n      </geometry>\r\n    </visual>\r\n    <collision>\r\n      <geometry>\r\n        <box size="0.2 0.1 0.05"/>\r\n      </geometry>\r\n    </collision>\r\n    <inertial>\r\n      <mass value="2"/>\r\n      <inertia ixx="0.01" ixy="0" ixz="0" iyy="0.01" iyz="0" izz="0.01"/>\r\n    </inertial>\r\n  </link>\r\n\r\n  <joint name="left_ankle_joint" type="fixed">\r\n    <parent link="left_knee"/>\r\n    <child link="left_ankle"/>\r\n    <origin xyz="0 0 -0.2" rpy="0 0 0"/>\r\n  </joint>\r\n\r\n  <link name="right_hip">\r\n    <visual>\r\n      <geometry>\r\n        <box size="0.1 0.1 0.1"/>\r\n      </geometry>\r\n    </visual>\r\n    <collision>\r\n      <geometry>\r\n        <box size="0.1 0.1 0.1"/>\r\n      </geometry>\r\n    </collision>\r\n    <inertial>\r\n      <mass value="5"/>\r\n      <inertia ixx="0.02" ixy="0" ixz="0" iyy="0.02" iyz="0" izz="0.02"/>\r\n    </inertial>\r\n  </link>\r\n\r\n  <joint name="right_hip_joint" type="fixed">\r\n    <parent link="base_link"/>\r\n    <child link="right_hip"/>\r\n    <origin xyz="0 -0.1 -0.3" rpy="0 0 0"/>\r\n  </joint>\r\n\r\n  <link name="right_knee">\r\n    <visual>\r\n      <geometry>\r\n        <box size="0.1 0.1 0.3"/>\r\n      </geometry>\r\n    </visual>\r\n    <collision>\r\n      <geometry>\r\n        <box size="0.1 0.1 0.3"/>\r\n      </geometry>\r\n    </collision>\r\n    <inertial>\r\n      <mass value="3"/>\r\n      <inertia ixx="0.05" ixy="0" ixz="0" iyy="0.05" iyz="0" izz="0.01"/>\r\n    </inertial>\r\n  </link>\r\n\r\n  <joint name="right_knee_joint" type="fixed">\r\n    <parent link="right_hip"/>\r\n    <child link="right_knee"/>\r\n    <origin xyz="0 0 -0.2" rpy="0 0 0"/>\r\n  </joint>\r\n\r\n  <link name="right_ankle">\r\n    <visual>\r\n      <geometry>\r\n        <box size="0.2 0.1 0.05"/>\r\n      </geometry>\r\n    </visual>\r\n    <collision>\r\n      <geometry>\r\n        <box size="0.2 0.1 0.05"/>\r\n      </geometry>\r\n    </collision>\r\n    <inertial>\r\n      <mass value="2"/>\r\n      <inertia ixx="0.01" ixy="0" ixz="0" iyy="0.01" iyz="0" izz="0.01"/>\r\n    </inertial>\r\n  </link>\r\n\r\n  <joint name="right_ankle_joint" type="fixed">\r\n    <parent link="right_knee"/>\r\n    <child link="right_ankle"/>\r\n    <origin xyz="0 0 -0.2" rpy="0 0 0"/>\r\n  </joint>\r\n\r\n  \x3c!-- Gazebo plugins --\x3e\r\n  <gazebo reference="camera_link">\r\n    <sensor type="camera" name="rgb_camera">\r\n      <update_rate>30.0</update_rate>\r\n      <camera name="head">\r\n        <horizontal_fov>1.3962634</horizontal_fov>\r\n        <image>\r\n          <width>640</width>\r\n          <height>480</height>\r\n          <format>R8G8B8</format>\r\n        </image>\r\n        <clip>\r\n          <near>0.02</near>\r\n          <far>300</far>\r\n        </clip>\r\n        <noise>\r\n          <type>gaussian</type>\r\n          <mean>0.0</mean>\r\n          <stddev>0.007</stddev>\r\n        </noise>\r\n      </camera>\r\n      <plugin name="camera_controller" filename="libgazebo_ros_camera.so">\r\n        <frame_name>camera_link</frame_name>\r\n        <topic_name>camera/image_raw</topic_name>\r\n      </plugin>\r\n    </sensor>\r\n  </gazebo>\r\n\r\n  <gazebo reference="left_camera_link">\r\n    <sensor type="camera" name="left_camera">\r\n      <update_rate>30.0</update_rate>\r\n      <camera name="left">\r\n        <horizontal_fov>1.3962634</horizontal_fov>\r\n        <image>\r\n          <width>640</width>\r\n          <height>480</height>\r\n          <format>R8G8B8</format>\r\n        </image>\r\n        <clip>\r\n          <near>0.02</near>\r\n          <far>300</far>\r\n        </clip>\r\n      </camera>\r\n      <plugin name="left_camera_controller" filename="libgazebo_ros_camera.so">\r\n        <frame_name>left_camera_link</frame_name>\r\n        <topic_name>stereo/left/image_raw</topic_name>\r\n      </plugin>\r\n    </sensor>\r\n  </gazebo>\r\n\r\n  <gazebo reference="right_camera_link">\r\n    <sensor type="camera" name="right_camera">\r\n      <update_rate>30.0</update_rate>\r\n      <camera name="right">\r\n        <horizontal_fov>1.3962634</horizontal_fov>\r\n        <image>\r\n          <width>640</width>\r\n          <height>480</height>\r\n          <format>R8G8B8</format>\r\n        </image>\r\n        <clip>\r\n          <near>0.02</near>\r\n          <far>300</far>\r\n        </clip>\r\n      </camera>\r\n      <plugin name="right_camera_controller" filename="libgazebo_ros_camera.so">\r\n        <frame_name>right_camera_link</frame_name>\r\n        <topic_name>stereo/right/image_raw</topic_name>\r\n      </plugin>\r\n    </sensor>\r\n  </gazebo>\r\n\r\n  <gazebo reference="imu_link">\r\n    <sensor type="imu" name="imu_sensor">\r\n      <always_on>true</always_on>\r\n      <update_rate>100</update_rate>\r\n      <visualize>false</visualize>\r\n      <plugin filename="libgazebo_ros_imu_sensor.so" name="imu_plugin">\r\n        <topicName>imu</topicName>\r\n        <bodyName>base_link</bodyName>\r\n        <updateRateHZ>100.0</updateRateHZ>\r\n        <gaussianNoise>0.01</gaussianNoise>\r\n        <xyzOffset>0 0 0</xyzOffset>\r\n        <rpyOffset>0 0 0</rpyOffset>\r\n        <frameName>imu_link</frameName>\r\n      </plugin>\r\n    </sensor>\r\n  </gazebo>\r\n\r\n  \x3c!-- Joint state publisher --\x3e\r\n  <gazebo>\r\n    <plugin name="joint_state_publisher" filename="libgazebo_ros_joint_state_publisher.so">\r\n      <update_rate>30</update_rate>\r\n      <joint_name>left_shoulder_joint</joint_name>\r\n      <joint_name>left_elbow_joint</joint_name>\r\n      <joint_name>left_wrist_joint</joint_name>\r\n      <joint_name>right_shoulder_joint</joint_name>\r\n      <joint_name>right_elbow_joint</joint_name>\r\n      <joint_name>right_wrist_joint</joint_name>\r\n      <joint_name>neck_joint</joint_name>\r\n    </plugin>\r\n  </gazebo>\r\n\r\n</robot>\n'})}),"\n",(0,i.jsx)(r.h2,{id:"step-2-create-the-vla-integration-node",children:"Step 2: Create the VLA Integration Node"}),"\n",(0,i.jsxs)(r.p,{children:["Create ",(0,i.jsx)(r.code,{children:"vla_integration_node.py"}),":"]}),"\n",(0,i.jsx)(r.pre,{children:(0,i.jsx)(r.code,{className:"language-python",children:"#!/usr/bin/env python3\r\n\r\nimport rclpy\r\nfrom rclpy.node import Node\r\nfrom rclpy.action import ActionServer\r\nfrom sensor_msgs.msg import Image, CameraInfo\r\nfrom std_msgs.msg import String\r\nfrom geometry_msgs.msg import Pose, Point\r\nfrom cv_bridge import CvBridge\r\nimport cv2\r\nimport numpy as np\r\nimport spacy\r\nimport torch\r\nfrom torchvision import transforms\r\nfrom transformers import pipeline\r\nimport json\r\n\r\nclass VLAIntegrationNode(Node):\r\n    def __init__(self):\r\n        super().__init__('vla_integration_node')\r\n\r\n        # Initialize CV bridge\r\n        self.bridge = CvBridge()\r\n\r\n        # Initialize NLP model\r\n        try:\r\n            self.nlp = spacy.load(\"en_core_web_sm\")\r\n        except OSError:\r\n            self.get_logger().warn(\"spaCy model not found. Install with: python -m spacy download en_core_web_sm\")\r\n            self.nlp = None\r\n\r\n        # Initialize vision model (using a simple detector for this example)\r\n        self.object_detector = cv2.dnn.readNetFromDarknet(\r\n            \"config/yolo.cfg\",\r\n            \"config/yolo.weights\"\r\n        )  # In practice, you'd use a proper configuration\r\n\r\n        # Publishers and subscribers\r\n        self.image_sub = self.create_subscription(\r\n            Image,\r\n            'camera/image_raw',\r\n            self.image_callback,\r\n            10\r\n        )\r\n\r\n        self.command_sub = self.create_subscription(\r\n            String,\r\n            'voice_command',\r\n            self.command_callback,\r\n            10\r\n        )\r\n\r\n        self.action_pub = self.create_publisher(\r\n            String,\r\n            'robot_action',\r\n            10\r\n        )\r\n\r\n        self.feedback_pub = self.create_publisher(\r\n            String,\r\n            'vla_feedback',\r\n            10\r\n        )\r\n\r\n        # Internal state\r\n        self.latest_image = None\r\n        self.detected_objects = []\r\n        self.current_command = None\r\n\r\n        self.get_logger().info('VLA Integration Node initialized')\r\n\r\n    def image_callback(self, msg):\r\n        \"\"\"Process incoming camera image\"\"\"\r\n        try:\r\n            # Convert ROS image to OpenCV\r\n            cv_image = self.bridge.imgmsg_to_cv2(msg, \"bgr8\")\r\n            self.latest_image = cv_image\r\n\r\n            # Process image to detect objects\r\n            self.detected_objects = self.detect_objects(cv_image)\r\n\r\n            # Publish processed results\r\n            result_msg = String()\r\n            result_msg.data = f\"Detected {len(self.detected_objects)} objects\"\r\n            self.feedback_pub.publish(result_msg)\r\n\r\n        except Exception as e:\r\n            self.get_logger().error(f'Error processing image: {e}')\r\n\r\n    def command_callback(self, msg):\r\n        \"\"\"Process incoming voice command\"\"\"\r\n        self.current_command = msg.data\r\n        self.get_logger().info(f'Received command: {msg.data}')\r\n\r\n        # Process command with current vision data\r\n        if self.latest_image is not None:\r\n            self.process_vla_command(msg.data, self.latest_image)\r\n\r\n    def detect_objects(self, image):\r\n        \"\"\"Detect objects in image using simple method\"\"\"\r\n        # In a real implementation, you would use a proper object detection model\r\n        # For this example, we'll use a simple color-based detection as placeholder\r\n        detected = []\r\n\r\n        # Convert to HSV for color detection\r\n        hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\r\n\r\n        # Define color ranges for simple detection\r\n        colors = {\r\n            'red': ([0, 50, 50], [10, 255, 255]),\r\n            'blue': ([100, 50, 50], [130, 255, 255]),\r\n            'green': ([40, 50, 50], [80, 255, 255])\r\n        }\r\n\r\n        for color_name, (lower, upper) in colors.items():\r\n            mask = cv2.inRange(hsv, np.array(lower), np.array(upper))\r\n            contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\r\n\r\n            for contour in contours:\r\n                if cv2.contourArea(contour) > 1000:  # Filter small contours\r\n                    x, y, w, h = cv2.boundingRect(contour)\r\n                    center_x = x + w // 2\r\n                    center_y = y + h // 2\r\n\r\n                    detected.append({\r\n                        'name': color_name,\r\n                        'bbox': [x, y, x + w, y + h],\r\n                        'center': [center_x, center_y],\r\n                        'confidence': 0.8\r\n                    })\r\n\r\n        return detected\r\n\r\n    def parse_command(self, command):\r\n        \"\"\"Parse natural language command\"\"\"\r\n        if self.nlp is None:\r\n            # Simple fallback parsing\r\n            command_lower = command.lower()\r\n            parsed = {\r\n                'action': 'unknown',\r\n                'object': 'unknown',\r\n                'location': 'unknown'\r\n            }\r\n\r\n            if 'pick' in command_lower or 'grasp' in command_lower or 'take' in command_lower:\r\n                parsed['action'] = 'grasp'\r\n            elif 'put' in command_lower or 'place' in command_lower:\r\n                parsed['action'] = 'place'\r\n            elif 'go' in command_lower or 'move' in command_lower:\r\n                parsed['action'] = 'navigate'\r\n\r\n            # Extract object (simplified)\r\n            for word in command_lower.split():\r\n                if word in ['cup', 'book', 'ball', 'red', 'blue', 'green']:\r\n                    parsed['object'] = word\r\n                    break\r\n\r\n            return parsed\r\n\r\n        # Use spaCy for more sophisticated parsing\r\n        doc = self.nlp(command)\r\n\r\n        parsed = {\r\n            'action': None,\r\n            'object': None,\r\n            'location': None\r\n        }\r\n\r\n        # Extract action (verb)\r\n        for token in doc:\r\n            if token.pos_ == \"VERB\":\r\n                parsed['action'] = token.lemma_\r\n                break\r\n\r\n        # Extract object\r\n        for ent in doc.ents:\r\n            if ent.label_ in [\"OBJECT\", \"PRODUCT\", \"EVENT\"]:\r\n                parsed['object'] = ent.text\r\n                break\r\n\r\n        return parsed\r\n\r\n    def match_object(self, command_object, detected_objects):\r\n        \"\"\"Match command object to detected objects\"\"\"\r\n        if not detected_objects:\r\n            return None\r\n\r\n        # Simple matching based on color or name\r\n        for obj in detected_objects:\r\n            if command_object.lower() in obj['name'].lower():\r\n                return obj\r\n\r\n        # If no exact match, return the first detected object as fallback\r\n        return detected_objects[0] if detected_objects else None\r\n\r\n    def plan_action(self, parsed_command, matched_object):\r\n        \"\"\"Plan action based on command and detected objects\"\"\"\r\n        action_plan = {\r\n            'action_type': parsed_command['action'],\r\n            'target_object': matched_object,\r\n            'success': True,\r\n            'reasoning': []\r\n        }\r\n\r\n        if parsed_command['action'] == 'grasp':\r\n            if matched_object:\r\n                action_plan['reasoning'].append(f\"Found {matched_object['name']} at {matched_object['center']}\")\r\n                action_plan['target_pose'] = self.calculate_grasp_pose(matched_object)\r\n            else:\r\n                action_plan['success'] = False\r\n                action_plan['reasoning'].append(\"Target object not found in view\")\r\n\r\n        elif parsed_command['action'] == 'navigate':\r\n            # For navigation, we'd need a map and path planning\r\n            action_plan['reasoning'].append(\"Navigation action planned\")\r\n\r\n        elif parsed_command['action'] == 'place':\r\n            action_plan['reasoning'].append(\"Placement action planned\")\r\n\r\n        return action_plan\r\n\r\n    def calculate_grasp_pose(self, object_info):\r\n        \"\"\"Calculate grasp pose for an object\"\"\"\r\n        # Convert 2D image coordinates to 3D world coordinates (simplified)\r\n        # In practice, you'd use depth information or stereo vision\r\n        x_2d, y_2d = object_info['center']\r\n\r\n        # Simplified 3D pose calculation\r\n        pose = Pose()\r\n        pose.position.x = x_2d / 100.0  # Scale to reasonable units\r\n        pose.position.y = y_2d / 100.0\r\n        pose.position.z = 0.5  # Fixed height for this example\r\n\r\n        # Default orientation for grasping\r\n        pose.orientation.w = 1.0  # No rotation\r\n\r\n        return pose\r\n\r\n    def process_vla_command(self, command, image):\r\n        \"\"\"Process complete VLA command\"\"\"\r\n        try:\r\n            # 1. Parse the language command\r\n            parsed_command = self.parse_command(command)\r\n            self.get_logger().info(f'Parsed command: {parsed_command}')\r\n\r\n            # 2. Match to detected objects\r\n            matched_object = self.match_object(parsed_command['object'], self.detected_objects)\r\n            self.get_logger().info(f'Matched object: {matched_object}')\r\n\r\n            # 3. Plan the action\r\n            action_plan = self.plan_action(parsed_command, matched_object)\r\n            self.get_logger().info(f'Action plan: {action_plan}')\r\n\r\n            # 4. Execute or publish the action\r\n            if action_plan['success']:\r\n                action_msg = String()\r\n                action_msg.data = json.dumps({\r\n                    'action_type': action_plan['action_type'],\r\n                    'target_object': action_plan['target_object'],\r\n                    'target_pose': {\r\n                        'x': action_plan.get('target_pose', {}).position.x if action_plan.get('target_pose') else 0,\r\n                        'y': action_plan.get('target_pose', {}).position.y if action_plan.get('target_pose') else 0,\r\n                        'z': action_plan.get('target_pose', {}).position.z if action_plan.get('target_pose') else 0\r\n                    } if action_plan.get('target_pose') else None\r\n                })\r\n                self.action_pub.publish(action_msg)\r\n\r\n                feedback_msg = String()\r\n                feedback_msg.data = f\"Executing: {command} - Found {len(self.detected_objects)} objects, matched to {matched_object['name'] if matched_object else 'none'}\"\r\n                self.feedback_pub.publish(feedback_msg)\r\n            else:\r\n                feedback_msg = String()\r\n                feedback_msg.data = f\"Could not execute: {command} - {action_plan['reasoning']}\"\r\n                self.feedback_pub.publish(feedback_msg)\r\n\r\n        except Exception as e:\r\n            self.get_logger().error(f'Error processing VLA command: {e}')\r\n            feedback_msg = String()\r\n            feedback_msg.data = f\"Error processing command: {e}\"\r\n            self.feedback_pub.publish(feedback_msg)\r\n\r\ndef main(args=None):\r\n    rclpy.init(args=args)\r\n    vla_node = VLAIntegrationNode()\r\n\r\n    try:\r\n        rclpy.spin(vla_node)\r\n    except KeyboardInterrupt:\r\n        pass\r\n    finally:\r\n        vla_node.destroy_node()\r\n        rclpy.shutdown()\r\n\r\nif __name__ == '__main__':\r\n    main()\n"})}),"\n",(0,i.jsx)(r.h2,{id:"step-3-create-supporting-configuration-files",children:"Step 3: Create Supporting Configuration Files"}),"\n",(0,i.jsxs)(r.p,{children:["Create ",(0,i.jsx)(r.code,{children:"config/vla_params.yaml"}),":"]}),"\n",(0,i.jsx)(r.pre,{children:(0,i.jsx)(r.code,{className:"language-yaml",children:"vla_integration_node:\r\n  ros__parameters:\r\n    # Vision parameters\r\n    detection_threshold: 0.5\r\n    max_detection_objects: 10\r\n    image_processing_rate: 10.0  # Hz\r\n\r\n    # Language parameters\r\n    language_confidence_threshold: 0.7\r\n    command_timeout: 30.0  # seconds\r\n\r\n    # Action parameters\r\n    action_execution_timeout: 60.0  # seconds\r\n    safety_distance: 0.1  # meters\r\n    max_retries: 3\r\n\r\n    # Integration parameters\r\n    sync_window: 0.1  # seconds\r\n    uncertainty_threshold: 0.6\n"})}),"\n",(0,i.jsx)(r.h2,{id:"step-4-create-the-launch-file",children:"Step 4: Create the Launch File"}),"\n",(0,i.jsxs)(r.p,{children:["Create ",(0,i.jsx)(r.code,{children:"launch/vla_demo.launch.py"}),":"]}),"\n",(0,i.jsx)(r.pre,{children:(0,i.jsx)(r.code,{className:"language-python",children:"from launch import LaunchDescription\r\nfrom launch.actions import DeclareLaunchArgument, IncludeLaunchDescription\r\nfrom launch.conditions import IfCondition\r\nfrom launch.launch_description_sources import PythonLaunchDescriptionSource\r\nfrom launch.substitutions import LaunchConfiguration, PathJoinSubstitution\r\nfrom launch_ros.actions import Node\r\nfrom launch_ros.substitutions import FindPackageShare\r\n\r\ndef generate_launch_description():\r\n    ld = LaunchDescription()\r\n\r\n    # Launch arguments\r\n    use_sim_time = LaunchConfiguration('use_sim_time', default='true')\r\n    params_file = LaunchConfiguration('params_file', default='')\r\n\r\n    ld.add_action(DeclareLaunchArgument(\r\n        'use_sim_time',\r\n        default_value='true',\r\n        description='Use simulation (Gazebo) clock if true'))\r\n\r\n    ld.add_action(DeclareLaunchArgument(\r\n        'params_file',\r\n        default_value=PathJoinSubstitution([\r\n            FindPackageShare('my_robot_pkg'),\r\n            'config',\r\n            'vla_params.yaml'\r\n        ]),\r\n        description='Path to parameters file'))\r\n\r\n    # Launch Gazebo\r\n    gazebo = IncludeLaunchDescription(\r\n        PythonLaunchDescriptionSource([\r\n            PathJoinSubstitution([\r\n                FindPackageShare('gazebo_ros'),\r\n                'launch',\r\n                'gazebo.launch.py'\r\n            ])\r\n        ]),\r\n    )\r\n    ld.add_action(gazebo)\r\n\r\n    # Spawn robot in Gazebo\r\n    spawn_entity = Node(\r\n        package='gazebo_ros',\r\n        executable='spawn_entity.py',\r\n        arguments=[\r\n            '-topic', 'robot_description',\r\n            '-entity', 'vla_humanoid',\r\n            '-x', '0', '-y', '0', '-z', '0.5'\r\n        ],\r\n        output='screen'\r\n    )\r\n    ld.add_action(spawn_entity)\r\n\r\n    # Robot state publisher\r\n    robot_state_publisher = Node(\r\n        package='robot_state_publisher',\r\n        executable='robot_state_publisher',\r\n        name='robot_state_publisher',\r\n        output='screen',\r\n        parameters=[{'use_sim_time': use_sim_time}]\r\n    )\r\n    ld.add_action(robot_state_publisher)\r\n\r\n    # Joint state publisher\r\n    joint_state_publisher = Node(\r\n        package='joint_state_publisher',\r\n        executable='joint_state_publisher',\r\n        name='joint_state_publisher',\r\n        parameters=[{'use_sim_time': use_sim_time}],\r\n        output='screen'\r\n    )\r\n    ld.add_action(joint_state_publisher)\r\n\r\n    # VLA integration node\r\n    vla_integration = Node(\r\n        package='my_robot_pkg',\r\n        executable='vla_integration_node',\r\n        name='vla_integration_node',\r\n        parameters=[\r\n            {'use_sim_time': use_sim_time},\r\n            PathJoinSubstitution([\r\n                FindPackageShare('my_robot_pkg'),\r\n                'config',\r\n                'vla_params.yaml'\r\n            ])\r\n        ],\r\n        output='screen'\r\n    )\r\n    ld.add_action(vla_integration)\r\n\r\n    # Simple voice command simulator (for testing)\r\n    voice_simulator = Node(\r\n        package='my_robot_pkg',\r\n        executable='voice_command_simulator',\r\n        name='voice_command_simulator',\r\n        parameters=[{'use_sim_time': use_sim_time}],\r\n        output='screen'\r\n    )\r\n    ld.add_action(voice_simulator)\r\n\r\n    # RViz for visualization\r\n    rviz_config = PathJoinSubstitution([\r\n        FindPackageShare('my_robot_pkg'),\r\n        'rviz',\r\n        'vla_demo.rviz'\r\n    ])\r\n    rviz = Node(\r\n        package='rviz2',\r\n        executable='rviz2',\r\n        name='rviz2',\r\n        arguments=['-d', rviz_config],\r\n        parameters=[{'use_sim_time': use_sim_time}]\r\n    )\r\n    ld.add_action(rviz)\r\n\r\n    return ld\n"})}),"\n",(0,i.jsx)(r.h2,{id:"step-5-create-a-simple-voice-command-simulator",children:"Step 5: Create a Simple Voice Command Simulator"}),"\n",(0,i.jsxs)(r.p,{children:["Create ",(0,i.jsx)(r.code,{children:"voice_command_simulator.py"}),":"]}),"\n",(0,i.jsx)(r.pre,{children:(0,i.jsx)(r.code,{className:"language-python",children:'#!/usr/bin/env python3\r\n\r\nimport rclpy\r\nfrom rclpy.node import Node\r\nfrom std_msgs.msg import String\r\nimport time\r\n\r\nclass VoiceCommandSimulator(Node):\r\n    def __init__(self):\r\n        super().__init__(\'voice_command_simulator\')\r\n\r\n        self.command_pub = self.create_publisher(\r\n            String,\r\n            \'voice_command\',\r\n            10\r\n        )\r\n\r\n        # Timer to send commands periodically\r\n        self.timer = self.create_timer(10.0, self.send_command)\r\n        self.command_index = 0\r\n\r\n        # Sample commands for demonstration\r\n        self.commands = [\r\n            "Pick up the red cup",\r\n            "Find the blue ball",\r\n            "Move to the kitchen",\r\n            "Grasp the green object",\r\n            "Put the object on the table"\r\n        ]\r\n\r\n        self.get_logger().info(\'Voice Command Simulator initialized\')\r\n\r\n    def send_command(self):\r\n        """Send a voice command periodically"""\r\n        if self.command_index < len(self.commands):\r\n            command = String()\r\n            command.data = self.commands[self.command_index]\r\n            self.command_pub.publish(command)\r\n            self.get_logger().info(f\'Sent command: {command.data}\')\r\n            self.command_index += 1\r\n        else:\r\n            # Reset after all commands are sent\r\n            self.command_index = 0\r\n\r\ndef main(args=None):\r\n    rclpy.init(args=args)\r\n    simulator = VoiceCommandSimulator()\r\n\r\n    try:\r\n        rclpy.spin(simulator)\r\n    except KeyboardInterrupt:\r\n        pass\r\n    finally:\r\n        simulator.destroy_node()\r\n        rclpy.shutdown()\r\n\r\nif __name__ == \'__main__\':\r\n    main()\n'})}),"\n",(0,i.jsx)(r.h2,{id:"step-6-build-and-run-the-vla-system",children:"Step 6: Build and Run the VLA System"}),"\n",(0,i.jsxs)(r.ol,{children:["\n",(0,i.jsx)(r.li,{children:"Build your ROS 2 package:"}),"\n"]}),"\n",(0,i.jsx)(r.pre,{children:(0,i.jsx)(r.code,{className:"language-bash",children:"cd ~/ros2_ws\r\ncolcon build --packages-select my_robot_pkg\r\nsource install/setup.bash\n"})}),"\n",(0,i.jsxs)(r.ol,{start:"2",children:["\n",(0,i.jsx)(r.li,{children:"Launch the VLA demonstration:"}),"\n"]}),"\n",(0,i.jsx)(r.pre,{children:(0,i.jsx)(r.code,{className:"language-bash",children:"ros2 launch my_robot_pkg vla_demo.launch.py\n"})}),"\n",(0,i.jsxs)(r.ol,{start:"3",children:["\n",(0,i.jsx)(r.li,{children:"In another terminal, you can also manually send commands:"}),"\n"]}),"\n",(0,i.jsx)(r.pre,{children:(0,i.jsx)(r.code,{className:"language-bash",children:"# Send a voice command\r\nros2 topic pub /voice_command std_msgs/String \"data: 'Pick up the red cup'\"\r\n\r\n# Monitor the robot's actions\r\nros2 topic echo /robot_action\r\n\r\n# Monitor feedback\r\nros2 topic echo /vla_feedback\n"})}),"\n",(0,i.jsx)(r.h2,{id:"expected-results",children:"Expected Results"}),"\n",(0,i.jsxs)(r.ul,{children:["\n",(0,i.jsx)(r.li,{children:"Robot model appears in Gazebo with appropriate sensors"}),"\n",(0,i.jsx)(r.li,{children:"Vision system detects colored objects in the environment"}),"\n",(0,i.jsx)(r.li,{children:"Language system parses natural language commands"}),"\n",(0,i.jsx)(r.li,{children:"Action system executes appropriate responses based on VLA integration"}),"\n",(0,i.jsx)(r.li,{children:"Robot successfully demonstrates simple VLA behaviors"}),"\n"]}),"\n",(0,i.jsx)(r.h2,{id:"troubleshooting",children:"Troubleshooting"}),"\n",(0,i.jsx)(r.p,{children:"If the VLA system doesn't work properly:"}),"\n",(0,i.jsxs)(r.ol,{children:["\n",(0,i.jsx)(r.li,{children:"Check that all required ROS 2 packages are installed"}),"\n",(0,i.jsx)(r.li,{children:"Verify that the robot model is properly configured with sensors"}),"\n",(0,i.jsx)(r.li,{children:"Ensure vision and language models are correctly loaded"}),"\n",(0,i.jsx)(r.li,{children:"Check that all topics are properly connected"}),"\n",(0,i.jsx)(r.li,{children:"Verify TF transforms are properly configured"}),"\n"]}),"\n",(0,i.jsx)(r.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,i.jsxs)(r.p,{children:["Continue to ",(0,i.jsx)(r.a,{href:"/humanoid-robotics-book/vla-integration/troubleshooting",children:"Troubleshooting"})," to learn about common VLA integration issues and solutions."]})]})}function d(n={}){const{wrapper:r}={...(0,t.RP)(),...n.components};return r?(0,i.jsx)(r,{...n,children:(0,i.jsx)(m,{...n})}):m(n)}},8453:(n,r,e)=>{e.d(r,{RP:()=>o});var i=e(6540);const t=i.createContext({});function o(n){const r=i.useContext(t);return i.useMemo(()=>"function"==typeof n?n(r):{...r,...n},[r,n])}}}]);